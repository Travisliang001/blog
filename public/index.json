[{"content":" Introduction Data Collection is a major step in every Machine Learning projects. The quality and quantity of data alone can greatly impact the results of the model. AutoAnnotator is an open-source automatic image annotator created using the Swift and Python. It\u0026rsquo;s an updated version of AutoLabelMe, the GUI I wrote using Tkinter library in Python for my Internship at Hubert Curien Laboratoire.\nIt matches the template provided by the user in an image and find same objects associating a bounding box and label for every object. AutoAnnotator uses Normalized Cross-Correlation to check whether two templates are similar. It is designed keeping in mind with the necessity of creating dataset for object detection purposes.\nYour browser doesn't support HTML video. Here is a link to the video instead. \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; Download | Installation | Tutorial | Examples | Change Log Installation Currently only for Macos, AutoAnnotator is released.\nPrerequisites MacOS Anaconda Distribution Python Python Library Requirements 1. Numpy\nuser@Mac ~ % conda install numpy or\nuser@Mac ~ % pip install numpy 2. Pandas\nuser@Mac ~ % conda install pandas 3. Opencv\nuser@Mac ~ % conda install -c conda-forge opencv or\nuser@Mac ~ % pip install opencv-python 4. PIL\nuser@Mac ~ % pip install Pillow Installtion of the python libraries is necessary for the app to run effectively. If you don\u0026rsquo;t have it installed or if the selected python environment in the app don\u0026rsquo;t have it installed, the app will install it automatically. This may delay results. Tutorial The first two steps are very important. These two steps are to be done only once.\n1. Change Python Enviroment (Important Step) 2. Path for Python Scripts (Important Step) 3. Add New Images 4. Draw Rectangle 5. Move Rectangle 6. Resize Rectangle 7. Match Rectangles Change Python Environment AutoAnnotator is written in Swift and Python and it only requires that Python is installed in your machine. You don\u0026rsquo;t need to install Swift. The libraries mentioned here are necessary for automatic box generation purposes. When you will first open AutoAnnotator, it will choose the default Python Environment which mayn\u0026rsquo;t have those libraries installed.\nYour system may have multiples python environments, this step will make sure you load the correct environment in the app.\nClick on \u0026#39;Settings\u0026#39; When the popover appears check the python path. The path may be like this\n/opt/homebrew/bin/python3 If you have installed the libraries in any anaconda enviroment then change the path to the anaconda python environment.\nClick on \u0026#39;Change Path\u0026#39; The anaconda environment paths look like these.\n~/anaconda/bin/python3 ~/anaconda/envs/your-environment-name/bin/python3 ~/anaconda/envs/your-environment-name/bin/python *Once this step is complete, check the path and restart the app for the changes to take effect. This step is to be done only once when you first open the app.\nPath for Python Scripts Some Python Scripts are provided with the installation file. You need to load the scripts in Settings/Load Scripts and choose the location where you have unzipped the downloaded file.\n*No need to restart after this step. This step is to be done only once when you open the app for the first time.\nAdd New Images To add one or multiple images, follow these steps\nClick on the + button in the middle or \u0026#39;New Image\u0026#39; button on the sidebar. Add one or multiple images Go to Next Image Click on the \u0026#39;next\u0026#39; and \u0026#39;previous\u0026#39; button to navigate through images. To move to your desired image, click on the current image name tab and a popover will appear where names of every open images are available. Click on the image name you want to load. Your browser doesn't support HTML video. Here is a link to the video instead. Draw Rectangle Click on \u0026#39;Draw Rectangles\u0026#39; on the sidebar and start drawing on the image. When drawing mode is enabled, you can\u0026rsquo;t move the image. To move the image, click on \u0026lsquo;Move image\u0026rsquo;.\nMove Rectangle Following video demonstrates how to change the box position.\nYour browser doesn't support HTML video. Here is a link to the video instead. Resize Rectangle To resize an image click on the red circle of the box and drag it to increase or decrease the size of the box. Your browser doesn't support HTML video. Here is a link to the video instead. Match Rectangles Examples ","date":"2023-01-11T00:00:00Z","title":"AutoAnnotator - MacOS Utility App","url":"/projects/autoannotator/"},{"content":"This is the first part of the VC Dimension and Neural Networks series. Basic definition and some examples of vc dimension of learning algorithms are explained here.\n1. Definition Let $S=\\{x_1,x_2,\\cdots,x_m\\}$ be the set of m random points from $\\mathbb{R}^d$ and $\\mathscr{H}$ be a class of $\\{-1,+1\\}$ valued functions on $\\mathbb{R}^d$ i.e $$\\forall \\\\, h \\in H, h:S \\to \\{0,1\\}^m$$ One such example is $h_1(x_i)=1 \\,\\forall \\, x_i \\in S$. In the context of Machine Learning, $\\mathscr{H}$ is the learning algorithm generally used for any classification task such as Logistic Regression etc. For each parameter $\\theta$ in Logistic Regression we get $h(\\theta;x) \\in \\mathscr{H}$. $\\textbf{Shattering:}$ $S$ is shattered by $\\mathscr{H}$ when all labeling of $S$ (with 0 and 1) can be computed by $\\mathscr{H}$ i.e for all labeling of $S$ denoted by $Y=\\{0,1\\}^m$, $\\exists \\\\, h\\in \\mathscr{H}$ such that the $\\sum_{i}|h(x_i)-y_i|=0$, where $y_i$ is the label of the ith point. Consider in Figure 1, $S$ is the set of three points, $\\mathscr{H}=\\{wx+b;w,b\\in \\mathbb{R}\\}$, for every possible labels of $S$ (colors of $S$), there is a line which achieves the classification task. So $\\mathscr{H}=\\{wx+b;w,b\\in \\mathbb{R}\\}$ shatters $S$ (from Figure 1). Figure 1: Shattering of 3 points in xy plane by lines. Red is for negative label and blue for positive label. $\\textbf{Vapnik-Chervonenkis (VC) dimension}$ of a hypothesis space($\\mathscr{H}$) is the cardinality of the largest $S$ such that $\\mathscr{H}$ shatters $S$. If $VC(\\mathscr{H})=n$, there there exists $n$ points which can be shattered by $\\mathscr{H}$ but no set of $(n+1)$ points can be shattered. 2. VC-dimension of class of Thresholds in $\\mathbb{R}$ Let $\\mathscr{H}$ be the class of thresholds i.e $\\forall h_a \\in \\mathscr{H}$, $$h_a(x)=\\begin{cases} 1 \\, \\text{if } x\\geq a \\\\ -1 \\, \\text{if } x\u003c a \\\\ \\end{cases}$$ Figure 2: Sample size of 2 can't be shattered by the class of thresholds. Red is for negative label and blue for positive label. The green bar is the threshold. For third example, there is no hypothesis which can predict the two points. When $|S|=1$, it can be shattered by $\\mathscr{H}$. For $|S|=2$ it's impossible to shatter since we can't find a threshold for third example (Figure 2). So $VC(\\mathscr{H})=1$ 3. VC-dimension of class of Intervals in $\\mathbb{R}$ Let $\\mathscr{H}$ be the class of intervals i.e $\\forall h([a,b];x) \\in \\mathscr{H}$, $$h_a(x)=\\begin{cases} 1 \\, \\text{if } x\\in [a,b] \\\\ -1 \\, \\text{if } x \\notin [a,b] \\\\ \\end{cases}$$ Figure 3: Shattering of 2 points by the class of intervals. Points belonging to the intervals are predicted positive. Sample of 2 points can be shattered by $\\mathscr{H}$ by taking the positive label in an interval.\nFigure 4: Shattering of 3 points can't be possible by the class of intervals. For the fourth setup, there is no interval. But sample of 3 points can\u0026rsquo;t be shattered by $\\mathscr{H}$, since there is no hypothesis that can predict the example where the middle value is negative (Figure 3). So $VC(\\mathscr{H})=2$.\n$\\textbf{General Case:}$ For hypothesis class of k non-intersecting intervals ($\\mathscr{H}^{k}$), VC dimension is $2k$ since every 2 points can be satisfied using one interval. But no sample with $2k+1$ points can be shattered since points with alternating positive and negative points can't be shattered by k intervals. (Figure 4) $\\textbf{Proof:}$ Let $S=\\{x_1,x_2,\\cdots,x_{2k+1} \\}$. Label S such that $x_{2i+1}=1, x_{2i}=-1, \\forall i=0,1,\\cdots,k$. Then there are $k+1$ positive examples. Since every positive example sits between two negative examples, every interval contains only one positive example. But there are only k intervals. So all the positive examples can't be covered. So $VC(\\mathscr{H}^{k})=2k+1$. Figure 4: For K=6, 2*6=12 points can be shattered by for 13 points, this configuration of labels can't be satisfied by the hypothesis space. 4. VC-dimension of class of axis-aligned rectangles in $\\mathbb{R}^2$ Let $\\mathscr{H}$ be the set of rectangles whose sides are parallel to the axes. Then it can shatter a set of 4 points. So $VC(\\mathscr{H})\\geq 4$ (Figure 5). Figure 5: Shattering of 4 points by axis-aligned rectangles. Red is for negative examples and blue is for positive examples. A set of 5 points can't be shattered. Let $S_5=\\{x_1,x_2,x_3,x_4,x_5 \\}$ be a set of 5 points. Let $x_i[1], x_i[2]$ is the x-coordinate and y-coordinate of the point $x_i$ respectively. $$ \\begin{align} y_1=argmax_{x_i}(x_1[1],x_2[1],\\cdots x_5[1])\\\\ y_2=argmin_{x_i}(x_1[1],x_2[1],\\cdots x_5[1])\\\\ y_3=argmax_{x_i}(x_1[2],x_2[2],\\cdots x_5[2])\\\\ y_4=argmin_{x_i}(x_1[2],x_2[2],\\cdots x_5[2])\\\\ \\end{align}$$ Then $\\{y_1,y_2,y_3,y_4\\} \\subset S_5$. Label $y_i=1, \\forall y_i$ and $S_5\\setminus \\{y_1,y_2,y_3,y_4\\}$ as -1. Then there is no rectangle that can satisfy this configuration. Since $x_k \\in S_5\\setminus \\{y_1,y_2,y_3,y_4\\}$ must be inside the rectangle. Figure 6: Calculation of $y_1,y_2,y_3,y_4$. So $VC(\\mathscr{H})= 4$ 5. VC-dimension of class of Line in $\\mathbb{R}^2$ $\\textbf{Theorem}:$ VC dimension of $\\mathscr{H}=\\{h;h(x)=sign(wx+b); w,b\\in \\mathbb{R}\\}$ in $\\mathbb{R}^2$ is 3 (Figure 1). $\\textbf{Proof}:$ Figure 1 shows a configuration of 3 points that can be shattered by $\\mathscr{H}$. So, $VC(\\mathscr{H}) \\geq 3$. Now we have to prove that no set of 4 points can be shattered by $\\mathscr{H}$. Consider the set $S_4=\\{x_1,x_2,x_3,x_4\\}$. Now if we create a convex hull of S and label the diagonal points as same class, then no line can't classify S. So $VC(\\mathscr{H})= 4$. 6. VC-dimension of class of hyperplane in $\\mathbb{R}^d$ $\\textbf{Theorem}:$ VC dimension of $\\mathscr{H}=\\{h;h(X)=sign(W^TX+b); W \\in \\mathbb{R}^d ,b\\in \\mathbb{R}\\}$ in $\\mathbb{R}^2$ is d.[1]\n$\\textbf{Proof}:$ We will first prove the lower bound. Let $S=\\{x_1,x_2,x_3,\\cdots,x_d\\} \\cup \\{x_{d+1}=0\\}$ and $\\{x_1,x_2,x_3,\\cdots,x_d\\}$ is the standard basis of $\\mathbb{R}^d$ $$\\begin{align} x_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\quad x_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\cdots x_d = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\end{align}$$ Let $Y=\\{y_1,y_2,\\cdots,y_d \\}$ is a class label for S with $y_i\\in \\{-1,+1\\}\\, \\forall \\, i$, then $$h(X)=sign(\\sum_i [x_i;1]\\hat{W}),\\hat{W}=[W;b]$$ can classify S. $$\\begin{align} [x_i;1] = \\begin{bmatrix} 0\\\\ 0 \\\\ \\vdots \\\\ i\\\\ \\vdots\\\\ 0\\\\ 1 \\end{bmatrix} \\end{align}$$ This is because the augmented set $\\{[x_1;1],[x_2;1],\\cdots, [x_d+1,1]\\}$ is linearly independent and since its cardinality is $d+1$, it's a basis of $\\mathbb{R}^{d+1}$. To prove that it's linearly independent, consider $\\sum_i^{d+1} [x_i;1]\\alpha_i =0$. $$ \\begin{bmatrix} 1 \u0026 0 \u0026\\cdots 1\\\\ 0 \u0026 1 \u0026\\cdots 1\\\\ \\vdots \\\\ 0\u0026 0 \u0026\\cdots 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_{d+1} \\end{bmatrix}= \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} $$ Then $\\alpha_1=0,\\alpha_2=0,\\cdots, \\alpha_{d+1}=0$. So the augmented set is linearly independent and a basis of $\\mathbb{R}^{d+1}$. So $$h(X):sign(\\sum_i [x_i;1]\\hat{W})=Y$$ will always have a closed form solution. (One trick is to solve $[X;1]\\hat{W}=Y \\implies \\hat{W}=[X;1]^{-1}Y, [X;1]=[[x_1;1] \\,[x_2;1]\\cdots \\,[x_{d+1};1] $). Then there exists a set of $d+1$ points that $\\mathscr{H}$ can shatter. So $VC(\\mathscr{H})\\geq d+1$ Now we will try to prove the upper bound ($VC(\\mathscr{H})\\leq d+1$). Let $S^{'}=\\{x_1,x_2,\\cdots, x_{d+2}\\}$ be the set of $d+2$ points that $\\mathscr{H}$ can shatter. Then we will get $W_i$ for every $2^{d+1}$ possible labeling such that $h(X)=sign([X;1]W_i)$. Let $W=[W_1^T \\, W_2^T \\, W_3^T \\cdots \\, W_{2^{d+2}}^T]$ be the vector containing all the $2^{d+2}$ weights. $$\\begin{align} h(X) = \\begin{bmatrix} [x_1;1] \\\\ [x_2;1] \\\\ \\vdots \\\\ [x_{d+2};1] \\end{bmatrix} \\begin{bmatrix} W_1^T W_2^T W_3^T W_4^T \\cdots W_{2^{d+2}}^T\\\\ \\end{bmatrix}\\\\ h(X)= \\begin{bmatrix} W_1^T[x_1;1] \u0026 W_2^T[x_1;1] \u0026 \\cdots \\cdots \u0026 W_{2^{d+2}}^T[x_1;1] \\\\ W_1^T[x_2;1] \u0026 W_1^T[x_2;1] \u0026 \\cdots \\cdots \u0026 W_{2^{d+2}}^T[x_2;1] \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots\\\\ \\vdots \u0026 \\vdots \u0026 \\quad \\quad \\quad \\quad \\quad\\ddots \u0026 \\vdots\\\\ W_1^T[x_{d+2};1] \u0026 W_1^T[x_{d+2};1] \u0026 \\cdots \\cdots \u0026 W_{2^{d+2}}^T[x_{d+2};1] \\end{bmatrix} \\end{align}$$ Then the row vectors of $h(X)$ are linearly independent. If $a^Th(X)=0$, then $$ \\begin{align} \\begin{bmatrix} a_1 \u0026 a_2 \u0026 \\cdots \u0026 a_{d+2} \\end{bmatrix}_{(1,d+2)} \\begin{bmatrix} W_1^T[x_1;1] \u0026 W_2^T[x_1;1] \u0026 \\cdots \\cdots \u0026 W_{2^{d+2}}^T[x_1;1] \\\\ W_1^T[x_2;1] \u0026 W_1^T[x_2;1] \u0026 \\cdots \\cdots \u0026 W_{2^{d+2}}^T[x_2;1] \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots\\\\ \\vdots \u0026 \\vdots \u0026 \\quad \\quad \\quad \\quad \\quad\\ddots \u0026 \\vdots\\\\ W_1^T[x_{d+2};1] \u0026 W_1^T[x_{d+2};1] \u0026 \\cdots \\cdots \u0026 W_{2^{d+2}}^T[x_{d+2};1] \\end{bmatrix}_{(d+2,2^{d+2})} \\end{align}$$ $$\\begin{align} = \\begin{bmatrix} \\sum_{i=1}^{d+2}a_iW_1^T[X_i;1] \\\\ \\sum_{i=1}^{d+2}a_iW_2^T[X_i;1] \\\\ \\vdots\\\\ \\sum_{i=1}^{d+2}a_{i}W_{d+2}^T[X_i;1] \\end{bmatrix} \\end{align} $$ Then the kth entry is $\\sum_{i=1}^{d+2}a_iW_k^T[X_i;1]$. Since we assumed that $\\mathscr{H}$ can shatter $S$ then $\\exists \\, k^{'}$ such that $sign(W_{k^{'}}^T[X_i;1])=sign(a_i) \\, \\forall \\, i$. So if $a_i\\neq 0$ for some $i$, then $\\sum_{i=1}^{d+2}a_iW_{k^{'}}^T[X_i;1] \u003e 0$, which is a contradiction, so $a_i=0 \\, \\forall \\, i$. So $rank(h(X))=d+2$. Since $h(X)=[X;1]W$, $rank(h(X))=min(rank([X;1],rank(W))) \\implies rank(h(X)) \\leq d+1$ since $[x_i;1] \\in \\mathbb{R^{d+1}}$ and the a set of $d+2$ vectors in $\\mathbb{R}^{d+1}$ is linearly dependent, so $rank([X;1]) \\leq d+1$. So it's a contradiction as we proved that $rank(h(X))=d+2$. Then our initial assumption that $S$ can be shattered by $\\mathscr{H}$ is wrong. The upper bound can also be proven using Radon's Theorem which states that any set $S \\subset R^{d}$ with $|S|=d+2$ can be partitioned into two disjoint subsets $S_1$ and $S_2$ whose convex hulls intersect which means it can't be separated by a hyperplane for all possible labeling, hence it can't be shattered by $\\mathscr{H}$. So $VC(\\mathscr{H})=d+1$. 7. Bibliography https://mlweb.loria.fr/book/en/VCdimhyperplane.html\n","date":"2022-09-18T00:00:00Z","title":"Part 1: VC Dimension - Definition and Examples","url":"/blog/vc-nn/introduction/"},{"content":" 1. What is Point Cloud? A Point Cloud is a set of points in 3D space which can represent the boundary or the whole object (including inside points). In a point cloud, the points are unordered and are not restricted by any grid which means a point cloud can be expressed in an infinite way (using translation). Each point can have 3D coordinates and feature vectors. $$ P={(X_i,F_i)}^{i=N}_{i=1}, X_i\\in\\mathbb{R}^3,F_i\\in\\mathbb{R}^d$$\n2. Properties of Point Cloud in $\\mathbb{R}^3$ Unordered: Unlike images or arrays, point cloud is unordered. It has no restriction to be confined within a boundary. This causes a problem for CNN type architecture to learn since CNN uses convolutional operations which requires ordered and regular array like representation of the input. Point cloud networks are generally invariant to the $N!$ number of permutations in input.\nIrregularity: Points are not sampled uniformly from an image which means different objects can have dense points while others sparse [1, 2]. This sometimes causes class imbalance problems in point cloud dataset.\nConnectedness: Since points are not connected like graph structure and neighbouring points contain meaningful spatial and geometry information of the object, networks must learn to pass information from points to points.\n3. Point Cloud Generation Point clouds are generated by 3D Scanners like time-of-flight sensors and depth cameras or photogrammetry software. Time-of-flight sensors use the reflected laser beams from sensors to the object to capture the surface of the object.\n4. Point Cloud Sampling Point Cloud Sampling is the method of choosing a subset of point clouds from a large point cloud set. Sampling methods were used in segmentation model to reduce the number of points for faster learning [RandLa-Net]. This is an essential step in the large-scale point cloud processing, since learning features for all the points can be time consuming. Instead, features can be learnt for small point clouds and for other points, it can be aggregated using neighboring features. There are different sampling algorithms available. Let $N$ be the number of points, $M$ is the sampled number of points chosen with $N\u003eM$, $D$ is the maximum number of points in a 3D voxel grid ($N\u003e\u003eD$) and $K$ is the number of nearest neighbour($N\u003e\u003eK$). $\\textbf{1. Heuristic Sampling}$ Grid Sampling: In Grid Sampling, a 3D voxel grid is used over the point cloud and each occupied voxels extract one point based on averages or most frequently occurring classes. This sampling results in a uniform sample. The time complexity of the grid sampling is $O(ND)$. By averaging the points on the surface, grid sampling loses smooth boundary information. Random Sampling: One of the simplest sampling methods, Random Sampling takes $M$ random points from a point cloud of $N$ points ($N\u003eM$). Time complexity is $O(M)$ which makes it efficient to use in large-scale point cloud networks. Farthest Point Sampling(FPS): It iteratively extracts set of points $P=\\{p_1,p_2,\\cdots,p_M \\}$ such that $p_j$ is the farthest point from the first $j-1$ points in $P$. The time complexity is $O(M^2N)$ which makes it unsuitable for large scale point cloud processing. Inverse Density Importance Sampling: In IDIS, density is calculated for every point by adding the distance between the point and its nearest neighbors. $$density(x)=\\sum_{y\\in KNN(x)} \\lVert x-y \\rVert_2^2$$. So $N$ points are reordered according to the inverse of the density and top $M$ points are selected which means lower density points are more likely to be chosen than high dense points. Time complexity is $O((K+N)logN)$. This sampling can control density but is sensitive to outliers and noise. $\\textbf{2. Learning Based Sampling}$ Generator Based Sampling: Generator Based Sampling(GS) learns to generate a small subset of point clouds from the original point cloud. For a point cloud set $P$ and a task $T$, GS tries to find $S \\subset P$ by minimizing the objective function $f$ such that $$S^*=argmin_{S}(f(T(S))$$. It is an end-to-end trainable model. But at inference stage, it uses FPS to match subsets with original point cloud. It takes up to 20 minutes to sample 10% of $10^6$ points. Gumbel Subset Sampling: Gumbel Subset Sampling[4] uses attention mechanism to choose a representative and task-specific subset of the point cloud. Given an input set $X_i \\in \\mathbb{R}^{N_i\\times c}$, the task is to choose a suitable $X_{i+1} \\in \\mathbb{R}^{N_{i+1}\\times c}, N_{i+1} \\leq N_i$ and $$X_{i+1}=y\\cdot softmax(WX_i^T), W \\in \\mathbb{R}^{N_{i+1}\\times N_i}$$ It is completely end-to-end learnable and can be used in any segmentation network. 5. Bibliography Anh Nguyen, Bac Le, 3D Point Cloud Segmentation - A Survey, 2013 6th IEEE Conference on Robotics, Automation and Mechatronics (RAM), 2013, pp. 225-230.\nCharles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas, PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 77-85.\nQingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, A. Trigoni, A. Markham, RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nJiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, Qi Tian, Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 3318-3327. 10.1109/CVPR.2019.00344. M. Fan. Variants of Seeded Region Growing. Image Processing, IET · June 2015\nHang Su, Subhransu Maji ,Evangelos Kalogerakis, Erik Learned-Miller. Multi-view Convolutional Neural Networks for 3D Shape Recognition. 2015 IEEE International Conference on Computer Vision (ICCV), 2015, pp. 945-953 Saifullahi Aminu Bello , Shangshu Yu, Cheng Wang. Review: deep learning on 3D point clouds. Remote Sensing 12, No. 11:1729. Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou. Tangent Convolutions for Dense Prediction in 3D. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Zhijian Liu, Haotian Tang, Yujun Lin, Song Han. Point-Voxel CNN for Efficient 3D Deep Learning. Proceedings of the 33rd International Conference on Neural Information Processing Systems 2019. Charles R. Qi, Li (Eric) Yi, Hao Su, Leonidas J. Guibas PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 5105–5114. H. Zhao, L. Jiang, C. -W. Fu and J. Jia PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 5560-5568, doi: 10.1109/CVPR.2019.00571. ","date":"2022-06-22T00:00:00Z","title":"Part 1 - Point Cloud Introduction","url":"/blog/pointcloud-series/introduction/"},{"content":"This is the second part of the VC Dimension and Neural Networks series. This part lists important theorems with proofs and explanations. To know the basics of VC dimension, check Part 1 - VC Dimension - Definition and Examples.\n1. Definitions $\\textbf{Dichotomy}:$ A dichotomy of $S=\\{x_1,x_2,\\cdots,x_m\\}$ induced by $h \\in \\mathscr{H}$ is a partition of S into two disjoint sets $S_1$ and $S_2$ such that $$h(x_i)=\\begin{cases} 1 \\, \\text{if} \\, x_i \\in S_1 \\\\ -1 \\, \\text{if} \\, x_i \\in S_2\\\\ \\end{cases}$$ Let $\\mathscr{H}(S)$ be the set of all labelings or dichotomies that can be realized by $\\mathscr{H}$. $$\\mathscr{H}(S)=\\{ \\{ h(x_1),h(x_2),\\cdots h(x_m) \\}; h \\in \\mathscr{H} \\}$$ If $|\\mathscr{H}(S)|=2^{|S|}$, then $S$ is shattered by $\\mathscr{H}$. Since $S$ is chosen randomly, we define $\\textbf{Growth Function}$ [1] $$\\mathscr{H}[m]=max\\{|\\mathscr{H}(S)|; |S|=m ; S \\subset R^m \\}$$ 2. Theorem 1 - Radon\u0026rsquo;s Theorem $\\textbf{Theorem 1 (Radon's Theorem):}$ For every set $S \\subset {R}^{d}$ with $|S|=d+2$, $S$ can be divided into two disjoint sets whose convex hulls intersect. $\\textbf{Proof:}$ Let $S=\\{x_1,x_2,\\cdots,x_{d+2}\\} \\subset R^{d}$. Then $S$ is a linearly dependent set. Let $S_{aug}=\\{[x_1;1],[x_2,;1]\\cdots,[x_{d+2};1]\\} \\subset R^{d+1}$ be the augmented set and it's also a dependent set. Then the equation $$\\sum_{i=1}^{d+2}a_i[x_i;1]=0$$ has non-zero solution (not all $a_i=0$) and $\\sum_{i=1}^{d+2}a_i=0$. Let I be the convex hull of $\\{x_i:a_i\u003e0\\}$ and J be the convex hull of $\\{x_i:a_i\\leq 0\\}$. $\\{x_i:a_i\u003e0\\} \\cap \\{x_i:a_i\\leq 0\\} =\\phi$. Let $A=\\sum_{i:x_i\\in I}a_i=-\\sum_{j:x_j\\in J}a_j$. Then $\\frac{1}{A}\\sum_{i=1}^{d+2}a_ix_i=0 \\implies \\sum_{i:x_i\\in I}\\frac{a_j}{A}x_j=-\\sum_{j:x_j\\in J}\\frac{a_j}{A}x_j$ So $p=\\sum_{i:x_i\\in I}\\frac{a_j}{A}x_j=-\\sum_{j:x_j\\in J}\\frac{a_j}{A}x_j$ So $p \\in I \\text{ and } p \\in J$. So, S can be divided into two disjoint sets whose convex hulls intersect. 3. Theorem 2 - $VC(\\mathscr{H}) \\leq log_2(|\\mathscr{H}|)$ $\\textbf{Theorem 2:}$ Let $\\mathscr{H}$ be a set of binary-valued functions and $\\mathscr{H}$ is finite. Then $VC(\\mathscr{H}) \\leq log_2(|\\mathscr{H}|)$. $\\textbf{Proof:}$ Let $VC(\\mathscr{H})=d$, then there exists a set of $d$ points such that for every $2^d$ possible labelings, $\\exists \\, h \\in \\mathscr{H}$ which can classify the points. Then the cardinality of $\\mathscr{H}$ must be greater than $2^d$. So $|\\mathscr{H}|\\geq 2^d \\implies d \\leq log_2(\\mathscr{H}) \\implies VC(\\mathscr{H}) \\leq log_2(\\mathscr{H})$.; 4. Theorem 3 - Sauer\u0026rsquo;s Lemma $\\textbf{Theorem 2:}$ Let $VC(\\mathscr{H})=d$ and $S$ be a set of $m$ points. Then $$\\mathscr{H}[m] \\leq \\Phi_{d}(m)=\\sum\\limits_{i=0}^{d} {m\\choose i}$$ $\\textbf{Proof:}$ We will prove it by induction. Base Case: For the base case, we have to prove for (1) $d=0$ and arbitrary $m$ and (2) $m=0$ and arbitrary $d$. When $VC(\\mathscr{H})=d=0$, it means that no set of points can be shattered, so points can only be labeled one way. So, $\\mathscr{H}[m] = 1 \\leq \\Phi_{d}(m)$. When $m=0$ and $d$ is arbitrary, we can label $0$ points at most $1$ way. Inductive Step: Let us assume that $\\forall m^{'} \u003c m,d^{'} \u003c d$, this statement is true. Let $S=\\{x_1,x_2,\\cdots,x_m\\}$ and $VC(\\mathscr{H})=d$. Let $F$ be the set of functions defined only over $S$ such that $\\mathscr{H}(S)=F(S)=F$. Then for any set $S^{'} \\subset S$ that is shattered by $F$ can be shattered by $\\mathscr{H}$. So $VC(F)\\leq VC(\\mathscr{H})=d$. We now create two disjoint subsets of $F$. For every possible labelings of $\\{x_1,x_2,\\cdots,x_{m-1}\\}$ in F $$F_1=\\{ \\{f(x_1),f(x_2),\\cdots, f(x_{m-1}), 1 \\}; f \\in F \\}$$ $$F_2=F\\setminus F_1=\\{ \\{f(x_1),f(x_2),\\cdots, f(x_{m-1}), 0 \\}; f \\in F \\}$$ Then $\\forall f_2 \\in F_2$, $\\exists \\, f_1 \\in F_1$, such that $f_1(x_i)=f_2(x_i)$, $\\forall i=1,2,\\cdots, m-1$ and $f_1(x_m)=1, f_2(x_m)=0$. $$|\\mathscr{H}(S)|=|F(S)|=|F_1(S)|+|F_2(S)|$$ Figure 1: Let $S=\\{x_1,x_2,x_3,x_4\\}$. The $F_1(S)$ consists of all possible labelings for $\\{x_1,x_2,x_3\\} \\in F(S)$ and $x_4=1$ and if two hypotheses in $F$ label $\\{x_1,x_2,x_3\\}$ the same way, then the one which labels $x_4$ as $0$ will be in $F_2(S)$. Consider the case of $F_1$. It's obvious that $VC(F_1) \\leq VC(F) \\leq d$. Now, $|F_1(S)|=|F_1(S \\setminus \\{x_m\\})|$, since every labeling in $F_1(S)$ which is of the form $\\{f(x_1),f(x_2),f(x_3), \\cdots, f(x_{m-1}),1 \\}$, $$\\{f(x_1),f(x_2),f(x_3), \\cdots, f(x_{m-1}) \\} \\in F_1(S\\setminus \\{x_m\\})$$ So $|F_1(S)| \\leq |F_1(S\\setminus \\{x_m\\})|$. Let $\\{f^{'}(x_1),f^{'}(x_2),f^{'}(x_3), \\cdots, f^{'}(x_{m-1}) \\} \\in F_1(S\\setminus \\{x_m\\})$, then $$\\{f^{'}(x_1),f^{'}(x_2),f^{'}(x_3), \\cdots, f^{'}(x_{m-1}),1 \\} \\in F_1(S)$$ So, $|F_1(S\\setminus \\{x_m\\})| \\leq |F_1(S)| $. Then $$|F_1(S)| = |F_1(S\\setminus \\{x_m\\})| \\leq \\Phi_{d}(m-1)=\\sum\\limits_{i=0}^{d} {m-1\\choose i}$$ For $F_2$ $$|F_2(S)|=|F_2(S\\setminus\\{x_m\\})|$$ If $T$ is shattered by $F_2$, then $T\\cup \\{x_m\\}$ must be shattered by $F$ since $\\forall f_2 \\in F_2, \\exists \\, f_1 \\in F_1$ such that $f_1(x_i)=f_2(x_i)$, $\\forall \\, i=1,2,\\cdots, m-1$ and $f_1(x_m)=1, f_2(x_m)=0$ (In Figure 1, $F_2$ shatters $\\{x_3\\}$ then $F$ shatters $\\{x_3,x_4\\}$).$ VC(F_2) \\leq VC(F)-1 \\leq d-1$. $$ |F_2(S)| \\leq \\Phi_{d-1}(m-1)=\\sum\\limits_{i=0}^{d-1} {m-1\\choose i}$$ So, $|\\mathscr{H}(S)|=|F(S)|=|F_1(S)|+|F_2(S)| \\leq \\sum\\limits_{i=0}^{d} {m-1\\choose i} + \\sum\\limits_{i=0}^{d-1} {m-1\\choose i}$ $\\implies |\\mathscr{H}(S)| \\leq \\sum\\limits_{i=0}^{d} {m-1\\choose i} + \\sum\\limits_{i=0}^{d-1} {m-1\\choose i}$ $\\implies |\\mathscr{H}(S)| \\leq {m\\choose 0} + \\sum\\limits_{i=1}^{d} {m-1\\choose i} + \\sum\\limits_{i=1}^{d} {m-1\\choose i-1}$ (Since ${m\\choose 0}={m-1\\choose 0}$) $\\implies |\\mathscr{H}(S)| \\leq {m\\choose 0} + \\sum\\limits_{i=1}^{d} {m\\choose i}$ (Since ${m-1\\choose k}+{m-1\\choose k-1}={m\\choose k}$, which comes from $(1+X)^m=(1+X)^{m-1}(1+X)$. See here for more information.) $\\implies |\\mathscr{H}(S)| \\leq \\Phi_{d}(m)= \\sum\\limits_{i=0}^{d} {m\\choose i}$ So it proves the theorem. Corollary $\\textbf{Corollary 3.1:}$ $\\Phi_{d}(m)\u003c2^m$ if $d \u003c m$. $\\textbf{Proof:}$ We know, $\\Phi_{d}(m)=\\sum\\limits_{i=0}^{d} {m\\choose i}$ and $$ (1+x)^m=\\sum\\limits_{i=0}^{m} {m\\choose i}1^ix^{m-i}$$ Take x=1, then $$2^m=\\sum\\limits_{i=0}^{m}{m\\choose i}$$ So $\\Phi_{d}(m)=\\sum\\limits_{i=0}^{d} {m\\choose i}\u003c \\sum\\limits_{i=0}^{m} {m\\choose i}=2^m$ (if $d \u003c m$). Bibliography 8803 Machine Learning Theory\n","date":"2022-09-29T00:00:00Z","title":"Part 2: VC Dimension - Theorems and Lemmas","url":"/blog/vc-nn/theorems/"},{"content":" Point Cloud Segmentation Methods Point Cloud Segmentation is the task for grouping objects or assigning labels to every points in the point cloud. It is one of the most challenging tasks and a research topic in deep learning since point clouds are noisy, unstructured and lack connectedness property. All the methods are categorized into four categories.\n1. Edge Based Methods Edges describe the intrinsic characteristics of the boundary of any 3D object. Edge-based methods locate the points which have rapid changes in the neighborhood. Bhanu[1] proposed three approaches for detecting the edges of a 3D object. The first approach is calculating the gradient. Let $r(i,j)$ be the range value at $(i,j)$ position, the magnitude and the direction of edge can be calculated by $$m(i,j:0)=\\frac{r(i,j-k)+r(i,j+k)-2r(i,j)}{2k}$$ $$m(i,j;45)=\\frac{r(i-k,j+k)+r(i+k,j-k)-2r(i,j)}{2k\\sqrt2}$$ $$m(i,j;90)=\\frac{r(i-k,j)+r(i+k,j)-2r(i,j)}{2k}$$ $$m(i,j;135)=\\frac{r(i-k,j-k)+r(i+k,j+k)-2r(i,j)}{2k\\sqrt2}$$\nFor flat surfaces these values are zero, positive when edges are convex and negative when edges are concave. The maximum magnitude of gradient is $\\max m(i,j;\\theta)$ and the direction of edge is $argmax_{\\theta}$ $m(i,j;\\theta)$. Using threshold, points can be segmented. The second approach is fitting 3D lines to a set of points(i.e neighboring points) and detecting the changes in the unit direction vector from a point to the neighboring points. The third approach is a surface normal approach where changes in the normal vectors in the neighborhood of a point determine the edge point. Edge models are fast and interpretable but they are very sensitive to noise and sparse density of point clouds and lack generalization capability. Learning on incomplete point cloud structure with edge-based models does not give good accuracy. In Medical image datasets especially MRI data, the organ boundaries sometimes do not have high gradient points compared to CT data which means for every modality, we have to find new thresholds in edge-based methods.\n2. Region Based Methods Region-based methods use the idea of neighborhood information to group points that are similar thus finding similarly grouped 3D objects and maximizing the dissimilarity between different objects. Compared to edge-based methods, these methods are not susceptible to noise and outliers but they suffer from inaccurate border segmentation. There are two types of region-based methods.\nSeeded-region Methods(bottom up): Seeded region segmentation is a fast, effective and very robust image segmentation method. It starts the segmentation process by choosing manually or automatically in preprocessing step, a set of seeds which can be a pixel or a set of pixels and then gradually adding neighbouring points if certain conditions satisfy regarding similarity[1,5]. The process finishes when every point belongs to a region. Suppose there are N seeds chosen initially. Let $A=\\{A_1,A_2,\\cdots,A_N\\}$ be the set of seeds. Let T be the set of pixels that are not in any $A_i$ but is adjacent to at least a point in $A_i$. $$T = \\bigg\\{x\\notin \\bigcup_{i=1}^{i=N} A_i|nbr(x) \\cap \\bigcup_{i=1}^{i=N} A_i \\neq \\phi \\bigg\\}$$ where $nbr(x)$ is the neighbourhood points of x. At each step if $nbr(x) \\cap A_i \\neq \\phi$, then x is added into the region if certain conditions are met. One such condition can be checking the difference between intensity value of $x$ with the average intensity value of $A_i \\forall A_i \\text{ such that } nbr(x) \\cap A_i \\neq \\phi$. The region with minimum difference is assigned to the point. There are another method when greyvalues of any point is approximated by fitting a line i.e if a coordinate of any pixel/point $p$ is $(x,y)$, then greyvalue of $p$, $G(p)=b+a_1x+a_2y+\\epsilon$, where $\\epsilon$ is the error term. The new homogeneity condition is to find the minimum distance between average approximated greyvalue and the approximated greyvalue of $x$. Seeded-based segmentation is very much dependent upon the choice of seed points. Inaccurate choices often lead to under-segmentation or over-segmentation. Unseeded-region Methods(top-down): Unlike seeded-based methods, unseeded methods have a top-down approach. The segmentation starts with grouping all the points into one region. Then the difference between all the mean point values and chosen point value is calculated. If it is more than the threshold then the point is kept otherwise the point is different than the rest of the points and a new region is created and the point is added into the new region and removed from the old region. The challenges are over-segmentation and domain-knowledge which is not present in complex scenes[1]. 3. Attribute Based Methods Attribute-based methods use the idea of clustering. The approach is to calculate attributes for points and then use a clustering algorithm to perform segmentation. The challenges in these methods are how to find a suitable attribute that contains the necessary information for segmentation and to define proper distance metrics. Some of the attributes can be normal vectors, distance, point density, or surface texture measures. It is a very robust method but performs poorly if points are large-scale and attributes are multidimensional.[1]\n4. Deep Learning Based Methods The main challenge in point cloud segmentation is find good latent vector which can contain sufficient information for segmentation task. Deep Learning methods offers the best solution to learn good representations. Neural networks being a universal approximator can theoretically approximate the target function for segmentation. The following theorem justifies how MLPs can approximate the function for the segmentation task given enough neurons.\nTheorem 1: Given a set of point clouds $X=\\{\\{x_i\\}_{i=1}^{i=n},n\\in \\mathbb{Z}^+,x_i \\in [0,1]^m\\} $, let $f:X \\rightarrow R$ be a continuous function with respect to hausdorff distance($d_H(\\cdot,\\cdot)$).$\\forall \\epsilon \u003e 0, \\exists \\eta,$ a continuous function and a symmetric set function $g(x_1,x_2,\\cdots x_n)=\\gamma \\circ MAX$ such that $\\forall S\\subset X$. $$\\bigg|f(S)-\\gamma \\bigg(\\underset{x_i \\in S}{MAX}(\\eta(x_i)) \\bigg) \\bigg|\u003c\\epsilon$$ $\\gamma$ is a continuous function and $MAX$ is an elementwise max operation which takes an input $k$ number of vectors and return a vector with element wise maximum. In practice $\\gamma \\text{ and } \\eta$ are MLP [2].\nProof: The hausdorff distance is defined by $$d_H(x,y)=\\max\\bigg\\{\\sup\\limits_{x\\in X}(\\inf\\limits_{y\\in Y} d(x,y)), \\sup\\limits_{y\\in Y}(\\inf\\limits_{x\\in X} d(x,y))\\bigg\\}$$ Since $f$ is a continuous function from $Y$ to $R$ w.r.t hausdorff distance, so by definition of continuity $\\forall \\epsilon \u003e 0, \\exists \\delta_{\\epsilon} \u003e 0 $ such that if $S_1,S_2 \\subset X$ and $d_H(S_1,S_2)\u003c\\delta_{\\epsilon}$, then $|f(S_1)-f(S_2)|\u003c\\epsilon$. Let $K=\\lceil {\\frac{1}{\\delta_\\epsilon}}\\rceil, K\\in \\mathbb{Z}^+$. So $[0,1]$ is evenly divided into K intervals. Let $\\sigma(x)$ be defined by $$\\sigma(x) =\\frac{\\lfloor{Kx}\\rfloor}{K}, x \\in S$$ So $\\sigma$ maps a point to the left side of the interval it belongs to and $$|x-\\frac{\\lfloor{Kx}\\rfloor}{K}|=\\frac{Kx-\\lfloor{Kx}\\rfloor}{K}\u003c1/K\\leq \\delta_\\epsilon$$ Let $\\tilde{S}={\\sigma(x)},x\\in S$, then $$|f(S) - f(\\tilde{S})|\u003c\\epsilon$$ Since $d_H(S,\\tilde{S})\\leq \\delta_\\epsilon.$ Let $\\eta_k(x)=e^{-d(x,[\\frac{k-1}{k},\\frac{k}{K}])}$ be the indicator function where $d(x,I)$ is the point to set distance $$d(x,I)=\\inf\\limits_{y \\in I}d(x,y)$$ $d(x,I)=0$, if $x\\in I$, so $\\eta_k(x)=1 \\text{ if } x \\in [\\frac{k-1}{k},\\frac{k}{K}].$ Let $\\eta(x)=[\\eta_1(x);\\eta_2(x);\\cdots;\\eta_n(x)]$. Since there are K intervals we can define K functions $v_j:\\mathbb{R}^n \\rightarrow \\mathbb{R}, \\forall j=1,\\cdots,K$ such that $$v_j(x_1,x_2,x_3,\\cdots,x_n)=\\max\\{\\eta_j(x_1),\\cdots,\\eta_j(x_n)\\}$$ So $v_j$ denotes if any points from $S$ occupy the $jth$ interval. Let $v=[v_1;v_2,\\cdots,;v_n]$. So $v:R^n\\rightarrow [0,1]^K$. Let $\\tau:[0,1]^K \\rightarrow X$ be defined by $$\\tau(v(x_1,x_2,\\cdots,x_n))=\\bigg\\{\\frac{k-1}{K}: v_k \\geq 1 \\bigg\\}$$ So $\\tau$ denotes the lower bound of any interval if it contains any point from $S$. In this respect, $\\tau(v) \\equiv \\tilde{S}$. Let $range(\\tau(v))=S_{\\tau}, d_H(S_{\\tau},S)\u003c\\frac{1}{K} \\leq \\delta_{\\epsilon}$ $$|f(\\tau(v(x_1,x_2,\\cdots,x_n)))-f(S)|\u003c\\epsilon$$ Let $\\gamma:\\mathbb{R}^K \\rightarrow R$ be a continuous function such that $\\gamma(v)=f(\\tau(v))$.Now $$\\gamma(v(x_1,x_2,\\cdots,x_n))=\\gamma (MAX)(\\eta(x_1),\\cdots,\\eta(x_n))$$ So $f$ can be approximated by a continuous($\\gamma$) and a symmetric function($MAX$).In practice, $\\gamma \\text{ and } \\eta$ can be approximated by MLP.\nThe DL methods for point cloud segmentation can be divided into following ways.\nA. Projection-Based Networks Following the success of 2d CNNs, projection-based networks use the projection of 3D point clouds into 2d images from various views/angles. Then 2D CNN techniques are applied to it to learn feature representations and finally features are aggregated with multi-view information for final output [6,7]. In [8], tangent convolutions are used. For every point, tangent planes are calculated and tangent convolutions are based on the projection of local surface geometry on the tangent plane. This gives a tangent image which is an $l\\times l$ grid where 2d convolutions can be applied. Tangent images can be computed even on a large-scale point cloud with millions of points. Compared to voxel-based models, multi-view models perform better since 2D CNN is a well-researched area and multi-view data contain richer information than 3D voxels even after losing depth information. The main challenges in multi-view methods are the choice of projection plane and the occlusion which can affect accuracy.\nB. Voxel-Based Networks Voxel-based methods convert the 3D point clouds into voxel-based images. Figure [1] shows an example. The points which make up the point cloud are unstructured and unordered but CNN requires a regular grid for convolution operation.\nFigure 1: Voxelization of a point cloud (Image from [9]) Voxelization is done in the following steps. A bounding box of the point cloud is calculated which defines the entire space that is to be divided. Then the space is divided into a fixed-size grid. Each grid is called 3D cuboids. The point cloud is divided into different grids with each 3D cuboid containing several points and these 3D cuboids become voxels that represent the subset of points. Features are calculated from the subset of points inside a voxel. Voxelization creates quantization artifacts and loses smooth boundary information. It is a computationally expensive preprocessing step and memory footprints increase cubically due to the cubical growth of voxels. If voxel resolution is low, many points will belong to a voxel and will be represented by a single voxel so these points will not be differentiable . A point is differentiable if it exclusively occupies one voxel grid. Figure 2 summarizes the memory requirements for if we want to retain higher number of differentiable points which will mean lower information loss [9]. To retain 90% of the differentiable points, GPU memory is more than 82 GB and voxel resolution is $128 \\times 128 \\times 128$ which is a huge computational overload. Figure 2: Voxelization and memory footprint (Image from [9])) After voxelization, 3D CNNs can be applied for learning features for segmentation (3d UNet). In a similar approach, Point-Voxel CNN [9] uses CNN and MLP bases fusion learning. It first voxelizes the point cloud and uses convolution for feature learning and then devoxelize the voxels for voxel-to-point mapping(i.e interpolation is used to create distinct features of a voxel for the points that belong to the voxel). The features of a point cloud are then aggregated with the features learned using MLP. Despite its remarkable advances in segmentation tasks in the medical domain in segmentation tasks, 3D CNNs have a lot of parameters and is computationally expensive. Reducing the input size causes the loss of important information. 3DCNN also requires a large number of training samples. C. Point-Based Networks Point-Based Networks work on raw point cloud data. They do not require voxelization or projection. PointNet is a breakthrough network that takes input as raw point clouds and outputs labels for every point. It uses permutation-invariant operations like pointwise MLP and symmetric layer, Max-Pooling layer for feature aggregation layer. It achieves state-of-the-art performance on benchmark datasets. But PointNet lacks local dependency information and so it does not capture local information. The max-pooling layer captures the global structure and loses distinct local information. Inspired by PointNet many new networks are proposed to learn local structure. PointNet++ extends the PointNet architecture with an addition of local structure learning method. The local structure information passing idea follows the three basic steps (1) Sampling (2) Grouping (3) Feature Aggregation Layer (Section 3.3.1.E lists some Feature Aggregation functions) to aggregate the information from the points in the nearest neighbors. Sampling is choosing $M$ centroids from $N$ points in a point cloud ($N\u0026gt;M$). Random Sampling or Farthest Point Sampling are two such methods for sampling centroids. Grouping refers to sample representative points for a centroid using KNN. It takes the input (1) set of points $N\\times(d+C)$, with $N$ is the number of points,$d$ coordinates and $C$ feature dimension and (2) set of centroids $N_1\\times d$. It outputs $N_1\\times K \\times (d+C)$ with $K$ is the number of neighbors. These points are grouped in a local patch. The points in the local patches are used for creating local feature representation for centroid points. These local patches work like receptive fields. Feature Aggregation Layer takes the feature of the points in the receptive field and aggregate them to output $N_1\\times(d+C)$. This process is repeated in a hierarchical way reducing the number of points as it goes deeper. This hierarchical structure enables the network to be able to learn local structures with an expanding receptive field. Most of the research in this field has gone into developing an effective feature aggregation layer to capture local structures. PointWeb creates a new module Adaptive Feature Adjustment to enhance the neighbor features by adding the information about the impact of features on centroid features and the relation between the points. It then combines the features and uses MLP to create new representations for centroid points. Despite their initial successes the following methods achieve higher performance due to their advanced local aggregation operators.\nD. Graph-Based Networks A point cloud is unstructured, unordered, and has no connectivity properties. But it can be transformed into a graph structure by adding edges to the neighbors. Graph structures are good for modeling correlation and dependency amidst points through edges. GNN based networks use the idea of graph construction, local structure learning using expanding receptive field, and global summary structure. PointGNN creates a graph structure using KNN and applies pointwise MLP on them followed by feature aggregation. It updates vertex features along with edge features at every iteration. Landrieu introduces super point graphs to segment large-scale point clouds. It first creates a partition of geometrically similar objects (i.e planes, cubes) in an unsupervised manner and applies graph convolutions for contextual segmentation. DGCNN introduces the Edge Convolution operation for dynamically updating the vertices and edges thus updating the graph itself. Ma creates a Point Global Context Reasoning module to capture the global contextual information from the output of any segmentation network by creating a graph from the output embedding vectors.\nE. Transformer and Attention-Based Networks Transformers and attention mechanism are a major breakthrough in NLP tasks. This has lead to research in attention mechanism in 2D CNN\\cite{attention}. Attention follows the following derivation. \\begin{equation} y_i=\\sum\\limits_{x_j\\in R(x_i)} \\alpha(x_i,x_j) \\odot \\beta(x_j) \\end{equation} where $\\odot$ is the Hadamard product, $R(x_i)$ is the local footprint of $x_i$ (i.e a receptive field, one such example can be nearest neighbors). $\\beta(x_j)$ produces a feature vector from $x_j$ that is adaptively aggregated using the vector of $\\alpha(x_i,x_j)$, where $\\alpha(x_i,x_j)=\\gamma(\\delta(x_i,x_j))$. $\\delta$ combines the features of $x_i$ and $x_j$ and $\\gamma$ explores the relationship between $x_i$ and $x_j$ expressively. In NLP, $\\gamma,\\delta \\text{ and }\\beta$ is known as Query, Key and Value. Some examples of $\\delta$ function can be $\\delta(x_i,x_j)=f_1(x_i)+f_2(x_j)$ $\\delta(x_i,x_j)=f_1(x_i)-f_2(x_j)$ $\\delta(x_i,x_j)=f_1(x_i)\\odot f_2(x_j)$ $\\delta(x_i,x_j)=[f_1(x_i);f_2(x_j)]$ ($;$ denotes concatenation) In Matrix Form: Let $P$ be the set of points in a point cloud ($P \\in \\mathbb{R)^{N\\times F}}$ where F is the feature channels). $Q,K\\in \\mathbb{R}^{N\\times C_k},V\\in \\mathbb{R}^{N\\times C_v}$. \\begin{equation}\\nonumber \\begin{split} \u0026amp; Q=PW_q,K=PW_k,V=PW_v\\ \u0026amp; Attention(Q,K,V)=softmax\\bigg(\\frac{QK^T}{\\sqrt{F_k}}\\bigg)V \\end{split} \\end{equation} The time complexity of original attention is $O(N^2C_v)$ and space complexity $O(N^2+NC_k+NC_v)$ which quadritically increases as $N$ increases. Attention mechanism can be categorized into two categories. Self Attention where Query, key and value is derived from the same input meaning model uses attention scores for making better representation from a single representation. $Cross Attention$ where Value and Key comes from an input and Query comes from another input. It\u0026rsquo;s used to query and learn conditional representation using the attention score of other feature vectors. In point cloud segmentation network, attention is generally used for local aggregation layer for giving adaptive weights to different features and can be used in point based or graph based networks. Point Transformer uses Equation 4 for local feature aggregation layer after extracting neighbors. Each of the component is approximated by an MLP. Furthermore it uses encoder-decoder like structure. In each layer of the encoder, points are downsampled by a certain factor and in decoders the points are upsampled with skip connections added for preventing information leak. 3D Medical Point Transformer uses an Edge Convolution module for computing query value. It uses Lambda Attention layer which is $$Attention(Q,K,V)=Q(softmax(K^T)V)$$ which has $O(NC_kC_v)$ time complexity and $O(NC_k+C_kC_v+C_kC_v)$ space complexity. Although it uses cost effective attention layer, 3DMedPT can\u0026rsquo;t perform large scale point cloud segmentation due to computational issues. Point Attention Network uses a new end-to-end subsampling method for downsampling the number of points and is permutation invariant and robust to noises and outliers.\n5. Bibliography Anh Nguyen, Bac Le, 3D Point Cloud Segmentation - A Survey, 2013 6th IEEE Conference on Robotics, Automation and Mechatronics (RAM), 2013, pp. 225-230.\nCharles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas, PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 77-85.\nQingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, A. Trigoni, A. Markham, RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nJiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, Qi Tian, Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 3318-3327. 10.1109/CVPR.2019.00344. M. Fan. Variants of Seeded Region Growing. Image Processing, IET · June 2015\nHang Su, Subhransu Maji ,Evangelos Kalogerakis, Erik Learned-Miller. Multi-view Convolutional Neural Networks for 3D Shape Recognition. 2015 IEEE International Conference on Computer Vision (ICCV), 2015, pp. 945-953 Saifullahi Aminu Bello , Shangshu Yu, Cheng Wang. Review: deep learning on 3D point clouds. Remote Sensing 12, No. 11:1729. Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou. Tangent Convolutions for Dense Prediction in 3D. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Zhijian Liu, Haotian Tang, Yujun Lin, Song Han. Point-Voxel CNN for Efficient 3D Deep Learning. Proceedings of the 33rd International Conference on Neural Information Processing Systems 2019. Charles R. Qi, Li (Eric) Yi, Hao Su, Leonidas J. Guibas PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 5105–5114. H. Zhao, L. Jiang, C. -W. Fu and J. Jia PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 5560-5568, doi: 10.1109/CVPR.2019.00571. Shi, Weijing and Rajkumar, Ragunathan (Raj) Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud.The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Landrieu, Loic and Simonovsky, Martin Large-Scale Point Cloud Semantic Segmentation with Superpoint Graphs.2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M. Dynamic Graph CNN for Learning on Point Clouds. ACM Transactions on Graphics (TOG). Ma, Yanni and Guo, Yulan and Liu, Hao and Lei, Yinjie and Wen, Gongjian Global Context Reasoning for Semantic Segmentation of 3D Point Clouds.. 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip HS and Koltun, Vladlen Point transformer.. Proceedings of the IEEE/CVF International Conference on Computer Vision. Jianhui Yu, Chaoyi Zhang, Heng Wang, Dingxin Zhang, Yang Song, Tiange Xiang, Dongnan Liu, Weidong Cai 3D Medical Point Transformer: Introducing Convolution to Attention Networks for Medical Point Cloud Analysis.. arXiv - CS - Computer Vision and Pattern Recognition (IF), Pub Date : 2021-12-09, DOI: arxiv-2112.04863. Mingtao Feng, Liang Zhang, Xuefei Lin, Syed Zulqarnain Gilani, Ajmal Mian Point Attention Network for Semantic Segmentation of 3D Point Clouds.. ","date":"2022-06-28T00:00:00Z","title":"Part 2 - Point Cloud Segmentation","url":"/blog/pointcloud-series/segmentation/"},{"content":"This is the third part of the VC Dimension and Neural Networks series. This part discusses the important theorems for vc dimension of MLPs. Please read Part 2: VC Dimension - Theorems and Lemmas for the important theorems that will be used here.\n1. Important Notation Abbreviation Meaning $NN$ Neural Networks $MLP$ Multilayer Perceptrons 2. Theorem 1 - Separable Regions In this theorem we will see how MLPs divides a space into different regions and classify accordingly. ","date":"2022-09-30T00:00:00Z","title":"Part 3: VC Dimension - Neural Networks","url":"/blog/vc-nn/nn/"},{"content":"","date":"2023-08-10T00:00:00Z","title":"Categories","url":"/categories/"},{"content":"","date":"2023-08-10T00:00:00Z","title":"Conferences","url":"/categories/conferences/"},{"content":"","date":"2023-08-10T00:00:00Z","title":"Journals","url":"/categories/journals/"},{"content":" * Equal Contributions 🤩 Spotlight/Hightlight ","date":"2023-08-10T00:00:00Z","title":"My Publications","url":"/publications/"},{"content":"","date":"2023-08-10T00:00:00Z","title":"Papers","url":"/categories/papers/"},{"content":"","date":"2023-08-10T00:00:00Z","title":"Publication","url":"/categories/publication/"},{"content":"","date":"2023-01-11T00:00:00Z","title":"Computer Vision","url":"/categories/computer-vision/"},{"content":"","date":"2023-01-11T00:00:00Z","title":"Image Annotation","url":"/categories/image-annotation/"},{"content":"","date":"2023-01-11T00:00:00Z","title":"MacOS","url":"/categories/macos/"},{"content":"** No content for the project index. This file provides front matter for the project including the layout and boolean options. **\n","date":"2023-01-11T00:00:00Z","title":"My Projects","url":"/projects/"},{"content":"","date":"2023-01-11T00:00:00Z","title":"Object Detection","url":"/categories/object-detection/"},{"content":"","date":"2023-01-11T00:00:00Z","title":"Swift","url":"/categories/swift/"},{"content":" Download Resume Work Experience PhD Student Feb 2024 - German Center of Artificial Intelligence (DFKI), RPTU Kaiserslautern, Germany Project: Vision-Language Architectures for 3D Scene and Shape Reconstruction.\nStudent Researcher Jan 2023-Feb 2024 CVI2 Lab, SnT, University of Luxembourg Kirchberg, Luxembourg Project: 3D Shape Modelling using Deep Geometric and Language Models.\nThe primary task was to develop novel vision-language architectures for CAD language inference from the point cloud.\nResearch Intern Feb 2022-Jul 2022 Creatis, INSA Lyon Lyon, France Master Thesis Title: Learning Shapes For The Effective Segmentation of 3D Medical Images using Point Cloud.\nWorked on 3D medical image segmentation using Point Cloud under Prof. Razmig Kechichian, Julie Digne, and Sebastien Valette.\nResearch Intern Apr 2021-Aug 2021 Laboratoire Hubert Curien, Université Jean Monnet Saint-Étienne, France Project: Detector-Encoder Autoencoders for unsupervised decomposition into visual parts.\nWorked on reconstructing historical ornament vignettes under Prof. Rémi Emonet and Prof. Thierry Fournel.\nData Analyst Intern May 2020-Jul 2020 Accenture Digital India Project: Intelligent Inventory Planning\nWorked on automatic forecast hedging to cover demand gaps using AI.\nEducation PhD in Computer Science Feb 2024- Rheinland-Pfälzische Technische Universität (RPTU) Kaiserslautern, Germany M.Sc in Machine Learning and Data Mining Oct 2020 - Sep 2022 Université Jean Monnet Saint-Étienne, France Erasmus Exchange in Faculty of Engineering Science Sep 2021 - Feb 2022 KU Leuven Leuven, Belgium Masters M1 in Data Science Aug 2019 - Jul 2020 Chennai Mathematical Institute Siruseri, India B.Sc in Mathematics Jul 2016 - May 2019 Ramakrishna Mission Residential College Kolkata, India ","date":"2022-12-21T00:00:00Z","title":"Mohammad Sadil Khan","url":"/cv/"},{"content":"","date":"2022-12-20T00:00:00Z","title":"","url":"/search/"},{"content":"","date":"2022-12-20T00:00:00Z","title":"","url":"/travel_video/"},{"content":"","date":"2022-12-20T00:00:00Z","title":"Travelling","url":"/categories/travelling/"},{"content":"","date":"2022-12-20T00:00:00Z","title":"Videography","url":"/categories/videography/"},{"content":"","date":"2022-12-20T00:00:00Z","title":"Welcome","url":"/travel_video/"},{"content":"","date":"2022-09-30T00:00:00Z","title":"Computational Theory","url":"/categories/computational-theory/"},{"content":"","date":"2022-09-30T00:00:00Z","title":"Deep Learning","url":"/categories/deep-learning/"},{"content":"","date":"2022-09-30T00:00:00Z","title":"MLP","url":"/categories/mlp/"},{"content":"** No content below YAML for the blog _index. This file provides front matter for the listing page layout and sidebar content. It is also a branch bundle, and all settings under cascade provide front matter for all pages inside blog/. You may still override any of these by changing them in a page\u0026rsquo;s front matter.**\n","date":"2022-09-30T00:00:00Z","title":"My Blogs","url":"/blog/"},{"content":"","date":"2022-09-30T00:00:00Z","title":"Series","url":"/series/"},{"content":"","date":"2022-09-30T00:00:00Z","title":"VC Dimension","url":"/categories/vc-dimension/"},{"content":"","date":"2022-09-30T00:00:00Z","title":"VC Dimension and Neural Networks","url":"/series/vc-dimension-and-neural-networks/"},{"content":"** No content below YAML for the series _index. This file is a leaf bundle, and provides settings for the listing page layout and sidebar content.**\n","date":"2022-09-30T00:00:00Z","title":"VC Dimension and Neural Networks","url":"/blog/vc-nn/"},{"content":"","date":"2022-08-12T00:00:00Z","title":"Java","url":"/categories/java/"},{"content":"","date":"2022-08-12T00:00:00Z","title":"Software","url":"/categories/software/"},{"content":"Code PDF\n\u0026nbsp Oops! Your browser doesn't support PDFs!\nDownload Instead\n","date":"2022-08-12T00:00:00Z","title":"Swedish Crossword Puzzle App","url":"/projects/scpa/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Deep Learning","url":"/tags/deep-learning/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Graph","url":"/tags/graph/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Graph","url":"/categories/graph/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"MLP","url":"/tags/mlp/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Point Cloud","url":"/tags/point-cloud/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Point Cloud","url":"/series/point-cloud/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Point Cloud","url":"/categories/point-cloud/"},{"content":"** No content below YAML for the series _index. This file is a leaf bundle, and provides settings for the listing page layout and sidebar content.**\n","date":"2022-06-28T00:00:00Z","title":"Point Cloud","url":"/blog/pointcloud-series/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Segmentation","url":"/tags/segmentation/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Segmentation","url":"/categories/segmentation/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Tags","url":"/tags/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Voxel","url":"/tags/voxel/"},{"content":"","date":"2022-06-28T00:00:00Z","title":"Voxel","url":"/categories/voxel/"},{"content":"","date":"2022-05-12T00:00:00Z","title":"Decoder","url":"/categories/decoder/"},{"content":"","date":"2022-05-12T00:00:00Z","title":"Encoder","url":"/categories/encoder/"},{"content":" 1. Point Cloud A. Introduction A Point Cloud is a set of points in 3D space which can represent the boundary or the whole object (including inside points). In a point cloud, the points are unordered and are not restricted by any grid which means a point cloud can be expressed in an infinite way (using translation). Each point can have 3D coordinates and feature vectors. $$P={(X_i,F_i)}^{i=N}_{i=1}, X_i\\in\\mathbb{R}^3,F_i\\in\\mathbb{R}^d$$\nB. Properties of Point Cloud in $\\mathbb{R}^3$ $\\textit{Unordered:}$ Unlike images or arrays, point cloud is unordered. It has no restriction to be confined within a boundary. This causes a problem for CNN type architecture to learn since CNN uses convolutional operations which requires ordered and regular array like representation of the input. Point cloud networks are generally invariant to the $N!$ number of permutations in input. $\\textit{Irregularity:}$ Points are not sampled uniformly from an image which means different objects can have dense points while others sparse [1, 2]. This sometimes causes class imbalance problems in point cloud dataset. $\\textit{Connectedness:}$ Since points are not connected like graph structure and neighbouring points contain meaningful spatial and geometry information of the object, networks must learn to pass information from points to points. 2. RandLaNet - Architecture Large-scale point cloud segmentation is a challenging task because of huge computational requirements and effective embedding learning. RandLa-Net[3] is an efficient and lightweight neural architecture that segments every point in large-scale point clouds. It is an encoder-decoder-like architecture that uses random sampling to downsample the input point cloud in the encoder and upsample the point cloud in decoder blocks. It uses random sampling compared to other sampling methods because of faster computation. Although random sampling can discard key points necessary for efficient point cloud segmentation, RandLa-Net implements attention-based local feature aggregation to effectively share features of points that are removed into the neighbor points. Figure[1] is the architecture of RandLa-Net. Figure 1: RandLa-Net Architecture. FC is the fully connected layer, LFA is the localfeature aggregation, RS is random sampling, MLP is shared multilayer perceptron,US is upsampling and DP is dropout. (Image from [3]) A. Random Sampling Compared to other sampling methods, Random sampling is extremely fast (time complexity $O(N)$). It is invariant to any changes to the points as well as the permutation of points. The random-sampling block is added in encoder part. To compensate for the loss of information, the author has added LFA module. Figure 2: Random Sampling in RandLa-Net. The downsampling rate is a hyperparameter and has significant influence on model performance (Image from [3]) B. Architecture RandLa-Net consists of 4 encoder and 4 decoder layers (Figure 1). Each encoder layer consists of LFA modules (which is shown in the bottom panel of Figure 3). LFA modules aggregate the local features and gradually expands the receptive field to perform global feature passing. Every LFA module is followed by a random sampling step. Let the input shape be $N\\times d_n$, where $N$ is the number of points in the point clouds ($N\\approx 10^6 - 10^7$) and $d_n \\in \\mathbb{R^d},d\\geq3$). $d_n$ can contain the coordinates with other features like intensity, gradient or normal. $\\textbf{Positional Encoding:}$Since point clouds are unstructured, positional encoding layer embeds the positional information in an 8 dimensional vector ($3\\rightarrow 8$). This layer describes the location of a point by mapping the position/index of a point into a vector and assigning unique representation for every point. In this way, positional encoding layer makes the network more permutation-invariant. $\\textbf{Encoding Layer:}$ The encoding layer progressively reduces the number of points and increases the point features. The point cloud is downsampled at each encoding layer after the dilated residual block by downsampling factor 4. $$N\\rightarrow \\frac{N}{4} \\rightarrow \\frac{N}{4^2} \\rightarrow \\frac{N}{4^3} \\rightarrow \\frac{N}{4^4}$$ The per-point feature dimension is increased gradually. $$8 \\rightarrow 32 \\rightarrow 128 \\rightarrow 256 \\rightarrow 512$$ $\\textbf{Decoding Layer:}$ In each decoder layer, points are upsampled. In each encoder layer, when a point is removed, it is stored as a reference. In subsequent decoding layer, (i.e the layer with which a skip connection is added from an encoder in Figure 1 for each query reference point, KNN is used to find the one nearest neighbor in the input set of points. Afterwards, feature of the nearest point is copied to the target point. Subsequently, the feature maps are concatenated with the feature maps produced by corresponding encoding layers through skip connections. Then a shared MLP is applied to the concatenated feature maps. Shared MLP means same MLP network for every point in the input point cloud. $\\textbf{Final Output Layer:}$ The segmentation label is predicted through three fully connected layers $(N,64) \\rightarrow (N,32) \\rightarrow (N,C)$, where $C$ is the number of classes. 3. RandLaNet - LFA The Local Feature Aggregation follows a three-step message passing system. Since point cloud don't have connectivity information, LFA ensures features are shared between points. In Figure 1, the LFA module in the first encoder transforms the feature vector ($8 \\rightarrow 32$) and random sampling removes 75% of the points. Let's take a point in the first encoder $(p,f),p\\in \\mathbb{R}^3,f\\in \\mathbb{R}^8$. Figure 3: RandLaNet Feature Sharing Let's take an overview of how this happens before diving deep into it. $\\textbf{1. Sampling:}$ The first step in message passing system is from which points we want to pass a message to the red point $p$ in Figure 3. K-Nearest Neighbor is used to find $K$ neighbor points (blue points) which will share its features with red point $p$. $\\textbf{2. Message Generation:}$ Once we choose the points, we need to generate the message to send from blue points to red point. For every point, $p_i$, we will generate a message $f_i$ by incorporating the distance and spatial information using an MLP. This MLP will give us the desired dimension of feature vector for $f_i,\\forall i=1,2,\\cdots,K$. $\\textbf{3. Message Passing:}$ There are several ways to share features from neighbor points. We can use MAX, AVG or SUM function. But the best method is use linear sum of the features $$f=\\sum\\limits_{i=1}^{6}\\alpha_if_i$$, with $\\alpha_i$ as learnable by the model. This $\\alpha_i$ is the attention score. It makes sure to give more weights during aggregation to points of similar nature or belonging to the same object. Figure 4: LFA Module Figure 4 is the detailed view of LFA module. It consists of three neural units (1) Local Spatial Encoding(LocSE) (2) Attentive Pooling (3) Dilated Residual Block. $\\textbf{A. Local Spatial Encoding}$\nLet $P=\\{p_1,p_2,\\cdots,p_n\\},p_i \\in \\mathbb{R}^3 \\text{ and } F=\\{f_1,f_2,\\cdots,f_n\\}, f_i \\in R^d$ be the point set and feature set accordingly. LSE units embed the features and the spatial information from the neighbourhood points. This helps the network learn the complex local geometrical structures with as increasing receptive field. For every point $p_i$, first K-Nearest Algorithm is used for finding $K$ neighbor points. Let the set of neighbor points, $N(p_i)=\\{p_1^{(i)},p_2^{(i)},\\cdots,p_K^{(i)}\\}$ and the set of features for the neighbor points be $N(f_i)=\\{f_1^{(i)},f_2^{(i)},\\cdots,f_K^{(i)}\\}$. At first positional features for every point in $N(p_i)$ is encoded as follows. (Figure 4) \\begin{equation} r_k^{(i)}=MLP\\bigg(p_i;p_k^{(i)};(p_i-p_k^{(i)});||p_i-p_k^{(i)}||\\bigg), r_k^{(i)} \\in \\mathbb{R}^r \\end{equation} $;$ is the concatenation layer and $||\\cdot||$ is the $l_2$ distance between neighbor and center points. $r_k^{(i)}$ not only just concatenates two positions but also the effect of one point on another point in terms of distance is also added. Once $r_k^{(i)} ,\\forall k=1,2,\\cdots,K$ is computed it is concatenated with corresponding features in $N(f_i)$. $$\\hat{F}=\\{\\hat{F}_1,\\hat{F}_2,\\cdots,\\hat{F}_i\\},\\hat{F_i}=\\{\\hat{f}_k^{(i)}\\}_{k=1}^{k=K}, \\hat{f}_k^{(i)}=\\{r_k^{(i)};f_k^{(i)}\\}$$ $\\textbf{B. Attentive Pooling}$\nAttentive pooling aggregates the set of neighboring point features $\\hat{F}$ with adaptive weights. Existing methods use mean or max pooling, resulting in the loss of important information. Attention mechanism will automatically learn important features. Given $\\hat{F_i}=\\{\\hat{f}_1^{(i)},\\cdots,\\hat{f}_k^{(i)}\\}$, first attention scores are computed using a shared MLP, $g$ such that \\begin{equation} s_k^{(i)}=g(\\hat{f}_k^{(i)},W) \\end{equation} where $W$ is the weight of the MLP. After learning the attention scores feature for point $p_i$, $f_i$ is updated with concatenated neighbor features. (Figure 4) \\begin{equation} \\hat{f}_i=MLP(\\sum\\limits_{k=1}^{K}(\\hat{f}_k^{(i)} \\odot s_k^{(i)})) \\end{equation} Together with LSE and Attentive pooling, the model learns informative features with geometric patterns for point $p_i$. $\\textbf{C. Dilated Residual Block}$ Since the point cloud is downsampled, it is necessary to expand the receptive field to preserve geometric details. Inspired by Resnet architecture, the author stacks several LSE and attentive pooling in one block before downsampling. In Figure 6, the red points observe $K$ features from neighboring points after the first LSE and Attentive Pooling layer and then in the next step it learns from $K^2$ features (See Figure 5). However, the more layers are added, the more the model is likely to be over-fitted. In the original paper (Figure 5), only two layers of LSE and Attentive pooling are used. Figure 5: Dilated Residual Block Figure 6: Illustration of dilated residual block which expands the receptive field at each step. 4. Conclusion $\\textbf{Advantages:}$\nThe main advantages of RandLa-Net are It is lightweight and achieves state-of-the-art results compared to existing methods. The random sampling method reduces the computation. The proposed attention-based Local Feature Aggregation (LFA) can expand into larger receptive fields using Local Spatial Encoding (LSE) with attentive pooling of point and neighbor features. The network consists of Shared MLP without any need of graph reconstruction or voxelization. The encoder-decoder architecture with downsampling aims to generate discriminative latent vectors using small samples which represent the objects of interest. $\\textbf{Disadvantages:}$\nThe random downsampling rate can influence the performance of the model. Reducing too many points will prevent the model from learning rich latent representations. Even though RandLaNet input allows addition of other features such as intensity, gradient, etc, it fails to learn local geometrical information. It learns the average shape of the object which causes over-segmentation. For more information, Thesis Report. ( Look at the Modified RandLa-Net with Feature Extractor and Voxel Segmentation Results) 5. Bibliography Anh Nguyen, Bac Le, 3D Point Cloud Segmentation - A Survey, 2013 6th IEEE Conference on Robotics, Automation and Mechatronics (RAM), 2013, pp. 225-230.\nCharles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas, PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 77-85.\nQingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, A. Trigoni, A. Markham, RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n","date":"2022-05-12T00:00:00Z","title":"RandLaNet","url":"/blog/randlanet/"},{"content":"** index doesn\u0026rsquo;t contain a body, just front matter above. See index.html in the layouts folder **\n","date":"2022-02-01T00:00:00Z","title":"Mohammad Sadil Khan","url":"/"},{"content":"** Contact page don\u0026rsquo;t contain a body, just the front matter above. See form.html in the layouts folder **\n","date":"2021-06-08T00:00:00Z","title":"Contact","url":"/contact/"},{"content":"","date":"2021-06-08T00:00:00Z","title":"Forms","url":"/form/"},{"content":"AutoLabelMe Project has been closed and the GUI has been rewritten in Swift and Python. Check the AutoAnnotator App. 1. Introduction Data Collection is a major step in every Machine Learning projects. The quality and quantity of data alone can greatly impact the results of the model. AutoLabelme is an open-source automatic image annotator created using the Tkinter library in Python. It\u0026rsquo;s an extension of LabelMe, the open-source Image Annotator available in Github LabelMe. It matches the template provided by the user in an image and find same objects associating a bounding box and label for every object. Autolabelme uses Normalized Cross-Correlation to check whether two templates are similar. It is designed keeping in mind with the necessity of creating dataset for object detection purposes and currently can only find objects in the cropped image i.e the search space to find same object is the space around the current template.\n2. Properties AutoLabelMe is simple to use. After LabelMe Manual Annotation just run AutoLabelMe. It is fast. The search space to the local neighbourhood of the template which makes it efficient for some projects. AutoLabelMe can identify rotated, scaled, horizontally and vertically flipped templates. There is no need for manually annotate rotated version of any template. It assigns a new label for the rotated templates with the rotaion information added in the label. AutoLabelMe also saves any meta information user stores during the LabelMe Manual Annotation step. For example, if for a bouding box, user assigns its label as \"0 This is a Sun\". AutoLabelMe will create a separate json file storing the new label for the bounding box and the meta information. This is helpful to save any additional information user wants for any object. 3. Tutorial Types of Windows Left Pane: Shows the image and matching for the current label. Red for the usual boxes. Blue for horizontally or vertically flipped boxes. Green for rotated. Although these are not final colors of the boxes. These are present only for checking. Top-Right Pane: Buttons. Bottom-Right Pane: Shows the image with all the matched templates. These image is helpful to recognise if every templates are matched or not. If any object is not annotated, it can be added using LabelMe. Functions of Every Buttons and Boxes Next Line $\u003e\u003e$: Template matching for next label. $\u003c\u003c$Previous Line: Template matching for previous label. $-$: Increases the threshold which results in less number of boxes. $+$: Decreses the threshold which results in detection of more boxes. Save Json: Saves a JSON file which can be read by LabelMe for further edits. Save Images: Save the cropped vignettes from the image. min: The minimum value of Rotation max: The maximum value of Rotation Rematch: Match again for the current label with the current setting. If min and max are provided, Rematch will match and activate rotation in template-matching. Correct Label:Transform all the boxes to red boxes (no flip). How to Run Open Terminal. Open Labelme. Write labelme or \"path/to/labelme\". Create one bounding box per label. Run AutoLabelme.py in Terminal python3 /path/to/AutoLabelme.py.. Open JSON and press Next Line $\u003e\u003e$ to start matching. The Left Pane will show the images with boxes for current Label. The smaller bottom-right pane is for showing the image with all the matched templates. Press $\u003c\u003c$Previous Line for viewing the matched boxes for the previous label. Press $+$ for more boxes and $-$ for less boxes. If you have rotated image, fill the rotation range or just enter min value. For example min=45 and max=90 will give the values 45,50,55,...90 or just enter min=45 which will only rotate the image once (45 degree). The search space value is by default 2 which means the algorithm will check for the templates from two heights up to two heights down in the original image. Choose any value from 1 to 15. For example if your template starts from coordinate (200,200) and height and width is 100 and 100 respectively and search space=2, the algorithm will search for the template from (0,0) to (400,400) in the image. If you select more than 15 than it's just heights-the value. For example, if you choose 100 in the previous example, the it will be (0,100) to (300,300) where the templates will be searched in the image. In any case, value from 1 to 15 is sufficient. Press Rematch button or Press Enter or Return in your keyboard. (Optional) Sometimes if the template is symmetric, the algorithm picks up some templates as flipped, to fix this, press Correct Label. Press Save Json to save a json file. Open the saved json file in Labelme. Labelme will show the matched templates. Edit it if necessary. Press Save Images in AutoLabelme if all the boxes are okay. This will save the matched templates in JPEG. 4. Future Work In V2, AutoLabelme will be used independently to annotate manually or automatically.\n","date":"2020-09-27T00:00:00Z","title":"AutoLabelMe","url":"/projects/autolabelme/"},{"content":"A Detector-Encoder Autoencoder, has two parts (Figure 1) - (1) the detection part in the front, $\\Phi(F_{d}(x))$, where $F_{d}$ corresponds to an encoding function performing RoI detection (a Detector-Encoder dedicated to representation learning in the scope of object detection, see Figure 2 and where $\\Phi$ corresponds to a RoI Pooling transformation (Figure 4) (2) the reconstruction part, $F_{r}(\\phi)$, where $\\phi=\\Phi(F_{d}(x))$), a Decoder responsible for image reconstruction.\n1. Detection Part In AutoEncoders, the encoder part encodes any input image from which the decoder part reconstructs the image. DEA is designed to reproduce the detected objects. Conventional AutoEncoders designed to perform well copy-and-paste of inputs, can generalize too much in some abnormal cases. To address this issue, we suggest to replace the encoder with a Single Shot Multibox Detector (SSD)[1] using the following feature maps for the purpose of detection with backbone classifier VGG-16 (Figure 2): Figure 2: SSD Architecture [1] Conv4_3: Size $38\\times 38\\times 512$ Conv7: Size $19\\times 19\\times 1024$ Conv8_2: Size $10\\times 10\\times 512$ Conv9_2: Size $5\\times 5\\times 256$ Conv10_2: Size $3\\times 3\\times 256$ Conv11_2: Size $1\\times 1\\times 256$ To optimize detector SSD, it is suggested to use an Intersection over Union (IoU) loss in replacement of $l_{1}$ loss, namely a Generalized IOU [2], Distance-IoU [3] and Efficient IOU [4].\nThe feature maps encode the characteristics of the objects of the dictionary. For each of the feature maps, SSD considers 4 or 6 default boxes per cell (Total cell=$(38\\times 38 \\times 4)+(19\\times 19\\times6)+(10\\times10\\times6)+(5\\times5\\times6)+(3\\times3\\times4)+(1\\times1\\times4)=8732$). The final output of SSD for each of the default boxes are: (1) its offset $(\\delta x,\\delta y,\\delta w,\\delta h)$ (2) a probability vector $(p_{0},p_{1},\\cdots,p_{n})$ where $p_{i}$ is the probability that the box contains an object of the $i\\mathit{th}$ class. Class $i$ for all $i \\in\\{1,\\cdots,n\\}$, corresponds to the $i\\mathit{th}$ object in the training dataset whereas class $0$ refers to the image background. A Region of Interest (RoI) is determined from the final predicted box by maximizing the probability values, which provides a confidence value with respect to the fact that it contains a certain object. The corresponding features are accessible by checking the corresponding index (Figure 4) and this gives us the information of which feature maps the RoI is from. RoIs are like the objects, of different sizes and aspect ratios. By adding some layers devoted to RoI resizing after SSD, RoIs are ready to be properly processed by the reconstruction part. 1) Bounding Box Regression: Bounding box regression is one of the most important components in object detection tasks. In conventional SSD, a $l_{n}$ norm is used during training to evaluate the performance of the detector with the IoU (Intersection over Union) metric: $IoU=\\frac{|B_{G}\\cap B_{P}|}{|B_{G} \\cup B_{P}|}$ where $B_{g}$ and $B_{d}$ are the ground and predicted bounding boxes, respectively. However there is no correlation between minimizing $l_{n}$ norm and improving the loss associated to the IoU metric, $L_{IoU}=1-IoU(B_{g},B_{d})$. Figure 3: Three cases where the $l_{2}$-norm distance between the representations of two rectangular bounding boxes, each given by the concatenation of the coordinates of two opposite corners, has the same value but IoU and GIoU metrics have very different values [2] In Figure 3, the predicted bounding box (black rectangle) and ground truth box (green rectangle) are each represented by their top-left and bottom-right corners (pointed by arrows), and whose the Cartesian coordinates are denoted as $(x_{1} , y_{1} , x_{2} , y_{2})$ and $(x_{1}' , y_{1}' , x_{2}' , y_{2}')$, respectively. For simplicity, let us assume that the distance, e.g. $l_{2}-norm$, between one of the corners of two boxes is fixed. Now, if the second corner lies on a circle with fixed radius centered on the ground truth box, then the $l_{2}$ loss between the ground truth box and the predicted bounding box is the same although their IoU values can be different depending upon the positions of top-right and bottom-left corners. So, using IOU-loss should be the best option since a bad detector will impact negatively in the reconstruction part. However IoU has two major issues as a metric, following from its definition. If two boxes do not overlap, then their IoU is zero, which does not give any indication whether they are close or far. In addition, in case of non-overlapping boxes, since their IOU is zero, the gradient is also zero, and loss $L_{IoU}$ cannot be optimized. A variant of this loss was suggested to address the weaknesses of the IoU metric: the Generalized IoU loss Generalized IOU , $L_{GIoU}=1-GIoU$ given by the metric defined by $GIoU=IoU-\\frac{|C\\setminus (B_{g}\\cup B_{P})|}{|B_{p}|}$ where $C$ is the convex hull of the union of bounding boxes $B_{g}, B_{P}$. Computing efficient, approximate versions were later proposed in Distance-IoU: The Distance IOU loss defined as $L_{DIoU}=1-DIoU$ where $$DIoU=IoU-\\frac{\\rho^{2}(b_{g},b_{p})}{c_C^{2}}$$, $\\rho$ is the euclidean distance, and $c_C$ is the length of the diagonal of convex hull $C$, The Efficient IOU loss Efficient IOU defined as $L_{EIoU}=1-EIoU$ where $$EIoU=IoU-\\frac{\\rho^{2}(b_{g},b_{p})}{c^{2}} -\\frac{\\rho^{2}(w_{g},w_{p})}{c_{w}^{2}}-\\frac{\\rho^{2}(h_{g},h_{p})}{c_{h}^{2}}, \\rho$$ is the euclidean distance, and ($b,w,h$ defines a box centered in point $\\mathbf{b}$ having width $w$ and height $h$, its diagonal length being denoted as $c$). Figure 4: RoI Alignment step in DEA architecture. 2) RoI Alignment: We used RoIAlign, first proposed in , which allows the extraction of a $k\\times k$ RoI where $k$ is a predefined integer value, from feature maps. For any $N\\times N$ RoI, RoIAlign divides the feature maps into $k^2$, $\\frac{N}{k} \\times \\frac{N}{k}$ regions, named RoI bins, in each of which is computed a single value: the maximum or the average of the values at four points determined at the end by a linear interpolation. 2. Reconstruction Part We constructed the Decoder with three fully connected linear layers. The feature maps obtained from xIoU-SSD are first transformed into $1 \\times 1 \\times 1024$ using Average Pooling with kernel $(5,5)$. Each of the transformed feature maps are flattened and fed into the linear layers which are as described below (See Fig. 1): Layer 1: Input 1024 $\\rightarrow$ Output 2048 (Activation Function: Relu). Layer 2: Input 2048 $\\rightarrow$ Output 4096 (Activation Function: Relu). Layer 2: Input 2048 $\\rightarrow$ Output 4096 (Activation Function: Relu). Layer 3: Input 4096 $\\rightarrow$ Output 16384 (Activation Function: Sigmoid) The output, any reconstructed object, is then reshaped into a $128 \\times 128 \\times 1$ image. 3. Bibliography Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg. SSD: Singe Shot Detector. Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, Silvio Savarese. Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression. Zheng Z., et al\nDistance-IoU Loss: Faster and Better Learning for Bounding Box Regression Zhang Y., et al\nFocal and Efficient IOU Loss for Accurate Bounding Box Regression He. k, et al\nMask RCNN ","date":"2020-09-27T00:00:00Z","title":"Detector-Encoder AutoEncoder","url":"/projects/dea/"},{"content":" 1. Why is $l_{n}$ not a good choice? Bounding Box Regression is the most important task in Object Detection. In conventional object detection networks, a $l_{n}$ norm is used during training to evaluate the performance of the detector with the IoU (Intersection over Union) metric: $$IoU=\\frac{|B_{G}\\cap B_{P}|}{|B_{G} \\cup B_{P}|}$$ where $B_{g}$ and $B_{d}$ are the ground and predicted bounding boxes, respectively. However there is no correlation between minimizing $l_{n}$ norm and improving the loss associated to the IoU metric [1], $$L_{IoU}=1-IoU(B_{g},B_{d})$$. Figure 1: Three cases where the $l_{2}$-norm distance between the representations of two rectangular bounding boxes, each given by the concatenation of the coordinates of two opposite corners, has the same value but IoU and GIoU metrics have very different values [1] In Figure 1, the predicted bounding box (black rectangle) and ground truth box (green rectangle) are each represented by their top-left and bottom-right corners (pointed by arrows), and whose the Cartesian coordinates are denoted as $(x_{1} , y_{1} , x_{2} , y_{2})$ and $(x_{1}' , y_{1}' , x_{2}' , y_{2}')$, respectively. For simplicity, let us assume that the distance, e.g. $l_{2}-norm$, between one of the corners of two boxes is fixed. Now, if the second corner lies on a circle with fixed radius centered on the ground truth box, then the $l_{2}$ loss between the ground truth box and the predicted bounding box is the same although their IoU values can be different depending upon the positions of top-right and bottom-left corners. So, using IOU-loss should be the best option since we will then minimize the evaluating metric. 2. IoU Loss [4] Generally, for two finite sample sets A and B, their IoU is defined as the intersection $(A \\cap B)$ divided by the union $(A \\cup B)$ of A and B. $IoU(A,B)=\\frac{|A \\cap B|}{|A \\cup B|}=\\frac{|A \\cap B|}{|A|+|B|-|A \\cup B|}$ For bounding box-level object detection, the target object is usually represented by a minimum Bbox rectangle in the 2D image. Base on this representation, the IoU computation between the ground bounding box $B_{g}=(x_{1} , y_{1} , x_{2} , y_{2} )$ and the predicted bounding box $B_{d}=(x_{1}^{\\prime} , y_{1}^{\\prime} , x_{2}^{\\prime} , y_{2}^{\\prime} )$ is defined as:- $ IoU(A,B)=\\frac{\\text{Area of overlap between $B_{g}$ and $B_{d}$}}{\\text{Area of union of $B_{g}$ and $B_{d}$}}=$ $\\frac{(max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime}))\\times (max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime}))}{(x_{2}-x_{1}) (y_{2}-y_{1})+(x_{2}^{\\prime}-x_{1}^{\\prime}) (y_{2}^{\\prime}-y_{1}^{\\prime})-(max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime})) (max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime}))}$ Usually, objects are labeled with axis-aligned BBoxes in the ROI dataset. By taking this kind of labels as ground truth, the predicted BBoxes are also axis-aligned rectangles. For this case, the IoU computation is very easy.\nA. Loss Function The IOU loss function[4] for the ground bounding box $B_{g}=(x_{1} , y_{1} , x_{2} , y_{2} )$ and the predicted bounding box $B_{d}=(x_{1}^{\\prime} , y_{1}^{\\prime} , x_{2}^{\\prime} , y_{2}^{\\prime} )$ is defined as $L_{IoU}=1-IoU(B_{g},B_{d})$ We have to prove that $L_{IoU}$ is a metric\nSince $0\\leq IoU \\leq 1, 0\\leq L_{IoU} \\leq 1$. So $L_{IoU}$ is non-negative. $L_{IoU}=0 \\text{ when } IoU(A,B)=1 \\implies$ $A$ and $B$ are the same rectangle. $IoU(A,B)=IoU(B,A) \\implies L_{IoU}(A,B)=L_{IoU}(B,A)$. So $L_{IoU}$ is symmetric. $L_{IoU}$ satisfies triangle inequality.[5] So $L_{IoU}$ is a metric. B. Differentiability of IoU Loss IOU loss is differentiable and can be backpropagated. Let $B_{g}=\\{x_{1},y_{1},x_{2},y_{2}\\}$ be the ground truth and $B_{d}=\\{x_{1}^{\\prime} , y_{1}^{\\prime} , x_{2}^{\\prime} , y_{2}^{\\prime}\\}$ be the predicted bounding box.\n$X= \\text{Area of } B_{g}=(x_{2}-x_{1}) \\times (y_{2}-y_{1})$ $X^{\\prime}= \\text{Area of } B_{d}=(x_{2}^{\\prime}- x_{1}^{\\prime})\\times (y_{2}^{\\prime}-y_{1}^{\\prime})$ $I=(max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime})) \\times (max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime}))$ $L_{IoU}=1-\\frac{I}{X+X^{\\prime}-I}=1-\\frac{I}{U}, \\text{where } U=X+X^{\\prime}-I$ $\\frac{\\partial L}{\\partial x^{\\prime}}=\\frac{I(\\Delta_{x^{\\prime}}X- \\Delta_{x^{\\prime}}I)-U\\Delta_{x^{\\prime}}I}{U^{2}}$ $\\frac{\\partial X}{\\partial x_{1}^{\\prime}}=- (y_{2}^{\\prime}-y_{1}^{\\prime}), \\frac{\\partial X}{\\partial x_{2}^ {\\prime}}=(y_{2}^{\\prime}-y_{1}^{\\prime}),\\frac{\\partial X} {\\partial y_{2}^{\\prime}}=(x_{2}^{\\prime}-x_{1}^{\\prime}), \\frac{\\partial X}{\\partial y_{1}^{\\prime}}=-(x_{2}^{\\prime}-x_{1}^ {\\prime})$ $\\frac{\\partial I}{\\partial x_{1}^{\\prime}}= \\begin{cases} (max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime})) \u0026 \\text{ if } x_{1}^{\\prime}\u003ex_{1}\\\\ 0 \u0026 { Otherwise } \\end{cases}$ $\\frac{\\partial I}{\\partial x_{2}^{\\prime}}= \\begin{cases} -(max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime})) \u0026 \\text{ if } x_{2}\u003ex_{2}^{\\prime}\\\\ 0 \u0026 { Otherwise } \\end{cases} $ $\\frac{\\partial I}{\\partial y_{1}^{\\prime}}= \\begin{cases} (max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime})) \u0026 \\text{ if } y_{1}^{\\prime}\u003ey_{1}\\\\ 0 \u0026 { Otherwise } \\end{cases}$ $\\frac{\\partial I}{\\partial y_{2}^{\\prime}}= \\begin{cases} -(max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime})) \u0026 \\text{ if } y_{2}\u003ey_{2}^{\\prime}\\\\ 0 \u0026 { Otherwise } \\end{cases}$ So $L_{IoU}$ can be directly used as the objective function to optimize. It is therefore preferable to use IoU as the objective function for 2D object detection tasks. Given the choice between optimizing a metric itself vs.a surrogate loss function, the optimal choice is the metric itself. 3. GIoU Loss However, IOU has two major issues as a metric and loss function.\nIf two boxes don\u0026rsquo;t overlap, then their IoU is zero, which doesn\u0026rsquo;t give any indication about the proximity of the two boxes. In case of non-overlapping boxes, since their Iou is zero, the gradient is also zero. So $L_{IoU}$ can\u0026rsquo;t be optimized. Generalized IoU (GIoU) addresses these weaknesses of IoU. A. Loss Function The Loss function for the Generalized IoU is defined as follows:-\nLet A and B be two boxes. Let C be the smallest enclosing box. $IoU=\\frac{|A\\cap B|}{|A \\cup B|}$. $GIoU=IoU-\\frac{|C\\setminus (A\\cup B)|}{|C|}$. $L_{GIoU}=1-GIoU$. Some of the properties of GIoU[1]:- Similar to $L_{IoU}, L_{GIoU}$ is also non-negative, symmetric and satisfies triangle inequality. So $L_{GIoU}$ is a metric.\nGIoU is a lower bound for IoU. $\\forall A,B, GIoU(A,B)\\leq IoU(A,B)$ and this lower bound becomes tighter when A and B have a stronger shape similarity i.e $\\lim_{A \\to B} GIoU(A,B)=IoU(A,B)$.\n$0\\leq IoU(A,B)\\leq 1 \\implies -1\\leq GIoU(A,B)\\leq 1$.\n$GIoU=1 \\text{ when } |A\\cap B|=|A\\cup B|$ i.e when A and B completely overlaps.\nGIoU tends to -1 as the ratio between occupying regions $|A\\cup B|$ and the smallest enclosing box C goes to zero, i.e $\\lim_{\\frac{|A\\cup B|}{C} \\to 0} GIoU(A,B)=-1 $\n$L_{GIoU}$ is differentiable.\nWhen IoU=0, i.e boxes don\u0026rsquo;t overlap, $L_{GIoU}=2-\\frac{|A\\cup B|}{|C|}$. By minimizing $L_{GIoU}$, we are maximizing $\\frac{|A\\cup B|}{|C|}$ ($0\\leq \\frac{|A\\cup B|}{|C|} \\leq 1$) which means we are maximizing the region of union $|A\\cup B|$ and minimizing the enclosing box area $|C|$, which will be possible if the predicted box goes to the ground truth box.\n4. DIoU and CIoU Loss IoU loss works only for overlapping boxes and the problem of gradient-vaninshing in case of non-overlapping boxes had been solved by GIoU Loss but GIoU loss has several limitations.\nGeneralized IoU tends to increase the size of the predicted bounding box to cover the target ground truth box. From Figure 2, we can see that when the predicted bounding box covers the ground truth box then $ L_{GIoU}=L_{IoU}$ (Since C=$max(A,B)\\implies C\\setminus(A\\cup B)=\\Phi$). $L_{GIoU}$ converges slowly. Figure 2: The green, black, blue, red represents the ground truth box, the anchor box, the predicted box at ith step when GIoU loss is used,the predicted box at ith step when DIoU loss is used respectively. GIoU tends to extend the box to cover the [2] A. Distance IoU (DIoU) Loss Function[2] Generally IoU-based loss functions can be defined as $$L=1-IoU+R(B,B^{gt})$$\nwhere $R(B,B^{gt})$ is a penalty term and $B_{gt}$ and $B$ are the ground truth box and predicted box. DIoU minimizes the normalized distance between the centre point of the two bounding boxes. The penalty term is $$R_{DIoU}=\\frac{\\rho^{2}(b,b_{gt})}{c^{2}}$$\nwhere $b$ and $b_{gt}$ denote the central points of $B$ and $B_{gt}$, and $c$ is the diagonal length of the smallest enclosing box. and $\\rho$ is the euclidean distance. So, $$L_{DIoU}=1-IoU+\\frac{\\rho^{2}(b,b_{gt})}{c^{2}}$$\n$L_{DIoU}$ is scale invariant. $L_{IoU}=L_{GIoU}=L_{DIoU}=0$ when two boxes are same. B. Complete IoU (CIoU) Loss Function[2] Complete IoU is based upon DIoU loss and considers the aspect ratio factor. The loss function for Complete IoU Loss function is $$L_{CIoU}=L_{DIoU}+\\alpha v$$\nwhere $v=\\frac{4}{\\pi^2}(tan^{-1}\\frac{w^{gt}}{h^{gt}} -tan^{-1}\\frac{w}{h})^2 $ and $\\alpha=\\frac{v}{(1-IOU)+v}$.\n5. Code import os import torch import math import torch.nn as nn import torchvision def calculate_iou(pred,true): \u0026#34;\u0026#34;\u0026#34;Functions to calculate IoU\u0026#34;\u0026#34;\u0026#34; ints_x_min=torch.max(true[:,0],pred[:,0]) ints_y_min=torch.max(true[:,1],pred[:,1]) ints_x_max=torch.min(true[:,2],pred[:,2]) ints_y_max=torch.min(true[:,3],pred[:,3]) width=torch.max((ints_x_max-ints_x_min),torch.tensor([0]).unsqueeze(0)) height=torch.max((ints_y_max-ints_y_min),torch.tensor([0]).unsqueeze(0)) area_intersection=torch.max(width*height, torch.tensor([0]).unsqueeze(0)) # Find Area of the Box True area_true=torch.mul((true[:,2]-true[:,0]),(true[:,3]-true[:,1])) # Find Area of the Box Pred area_pred=torch.mul((pred[:,2]-pred[:,0]),(pred[:,3]-pred[:,1])) # Find Area of the Union area_union=area_true+area_pred-area_intersection # Calculate IoU iou=area_intersection/area_union return iou,area_intersection,area_union class IoULoss(nn.Module): \u0026#34;\u0026#34;\u0026#34;Intersection over Union Loss\u0026#34;\u0026#34;\u0026#34; def __init__(self,losstype=\u0026#39;giou\u0026#39;): super(IoULoss, self).__init__() \u0026#34;\u0026#34;\u0026#34;losstype --\u0026gt; str. Type of IoU based Loss. \u0026#34;iou\u0026#34;,\u0026#34;giou\u0026#34;,\u0026#34;diou\u0026#34;,\u0026#34;ciou\u0026#34;,\u0026#34;eiou\u0026#34; are available\u0026#34;\u0026#34;\u0026#34; self.losstype =losstype def forward(self, pred, true): pred=torch.clamp(pred,min=0) true=torch.clamp(true,min=0) if self.losstype == \u0026#34;iou\u0026#34;: loss=torch.mean(1-calculate_iou(pred,true)[0]) elif self.losstype == \u0026#34;giou\u0026#34;: l_giou=1-calculate_iou(pred,true)[0]+self.penalty_giou(pred,true) loss=torch.mean(l_giou) elif self.losstype == \u0026#34;diou\u0026#34;: l_diou=1-calculate_iou(pred,true)[0]+self.penalty_diou(pred,true) loss=torch.mean(l_diou) elif self.losstype == \u0026#34;ciou\u0026#34;: l_ciou=1-calculate_iou(pred,true)[0]+self.penalty_ciou(pred,true) loss=torch.mean(l_ciou) elif self.losstype == \u0026#34;eiou\u0026#34;: l_eiou=1-calculate_iou(pred,true)[0]+self.penalty_eiou(pred,true) loss=torch.mean(l_eiou) return loss def penalty_giou(self, pred, true): # Find Area of the Smallest Enclosing Box box_x_min=torch.min(true[:,0],pred[:,0]) box_y_min=torch.min(true[:,1],pred[:,1]) box_x_max=torch.max(true[:,2],pred[:,2]) box_y_max=torch.max(true[:,3],pred[:,3]) area_c=(box_x_max-box_x_min)*(box_y_max-box_y_min) return (area_c-calculate_iou(pred,true)[2])/area_c def penalty_diou(self, pred, true): # Center point of the predicted bounding box center_x1 = (pred[:, 2] + pred[:, 0]) / 2 center_y1 = (pred[:, 3] + pred[:, 1]) / 2 # Center Point of the ground truth box center_x2 = (true[:, 2] + true[:, 0]) / 2 center_y2 = (true[:, 3] + true[:, 1]) / 2 inter_max_xy = torch.min(pred[:, 2:],true[:, 2:]) inter_min_xy = torch.max(pred[:, :2],true[:, :2]) # Bottom right corner of the enclosing box out_max_xy = torch.max(pred[:, 2:],true[:, 2:]) # Top left corner of the enclosing box out_min_xy = torch.min(pred[:, :2],true[:, :2]) # Distance between the center points of the ground truth and the predicted box inter_diag = (center_x2 - center_x1)**2 + (center_y2 - center_y1)**2 outer = torch.clamp((out_max_xy - out_min_xy), min=0) outer_diag = (outer[:, 0] ** 2) + (outer[:, 1] ** 2) return inter_diag/outer_diag def penalty_ciou(self, pred, true): w1 = pred[:, 2] - pred[:, 0] h1 = pred[:, 3] - pred[:, 1] w2 = true[:, 2] - true[:, 0] h2 = true[:, 3] - true[:, 1] v = (4 / (math.pi ** 2)) * torch.pow((torch.atan(w2 / h2) - torch.atan(w1 / h1)), 2) with torch.no_grad(): S = 1 - calculate_iou(pred,true)[0] alpha = v / (S + v) return self.penalty_diou(pred,true)+alpha*v def penalty_eiou(self, pred, true): w1 = pred[:, 2] - pred[:, 0] h1 = pred[:, 3] - pred[:, 1] w2 = true[:, 2] - true[:, 0] h2 = true[:, 3] - true[:, 1] # Bottom right corner of the enclosing box out_max_xy = torch.max(pred[:, 2:],true[:, 2:]) # Top left corner of the enclosing box out_min_xy = torch.min(pred[:, :2],true[:, :2]) # Width of the Smallest enclosing box C_w=(out_max_xy[:,0]-out_min_xy[:,0]) # Height of the smallest enclosing box C_h=(out_max_xy[:,1]-out_min_xy[:,1]) asp= torch.clamp((w2-w1)**2,min=0)/(C_w**2) + torch.clamp((h2-h1)**2,min=0)/(C_h**2) return self.penalty_diou(pred,true)+asp 6. Bibliography Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, Silvio Savarese.\nGeneralized Intersection over Union: A Metric and A Loss for Bounding Box Regression. Zheng Z., et al\nDistance-IoU Loss: Faster and Better Learning for Bounding Box Regression Zhang Y., et al\nFocal and Efficient IOU Loss for Accurate Bounding Box Regression J Yu, et al\nUnitBox: An Advanced Object Detection Network S Kosub A note on the triangle inequality for the Jaccard distance ","date":"2020-09-27T00:00:00Z","title":"IoU Losses","url":"/blog/iouloss/"},{"content":"** index doesn\u0026rsquo;t contain a body, just front matter above. See the header / main / sidebar folders to edit the index.md files **\n","date":"0001-01-01T00:00:00Z","title":"About","url":"/about/"},{"content":"My blogs are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n","date":"0001-01-01T00:00:00Z","title":"License","url":"/license/"}]