---
title: "Part 3: VC Dimension - Neural Networks"
weight: 3
subtitle: ""
excerpt: This is the third part of the <a href="/blog/vc-nn/">VC Dimension and Neural Networks</a> series. This part discusses the important theorems for vc dimension of MLPs.
Please read  <a href="/blog/vc-nn/theorems">Part 2: VC Dimension - Theorems and Lemmas</a> for the important theorems that will be used here.
date: 2022-09-30
draft: false
commentable: true
show-related: true
---

This is the third part of the <a href="/blog/vc-nn/">VC Dimension and Neural Networks</a> series. This part discusses the important theorems for vc dimension of MLPs.
Please read  <a href="/blog/vc-nn/theorems">Part 2: VC Dimension - Theorems and Lemmas</a> for the important theorems that will be used here.

## 1. Important Notation
<table style="width:40%">
  <tr>
    <th>Abbreviation</th>
    <th>Meaning</th>
  </tr>
  <tr>
    <td>$NN$</td>
    <td>Neural Networks</td>
  </tr>
  <tr>
    <td>$MLP$</td>
    <td>Multilayer Perceptrons</td>
  </tr>
</table>

## 2. Theorem 1 - Separable Regions
<p>
In this theorem we will see how <a href="#1-important-notation">MLPs</a> divides a space into different regions and classify accordingly.

 <video width="600" height="600" controls>
  <source src="two_lines_movie.m4v" type="video/mp4">
</video> 
</p>


